{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJksGDaWOs51",
        "outputId": "ba50adfd-94e1-480e-8ce8-9821abaeec96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e71C8kwEpXYb",
        "outputId": "32e6ede2-7ca3-4203-9926-f7616d66c95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader as TorchDL\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "G-StJ3vmO2yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 1.2.1 ENZYMES ──────────────────────────────────────────────────────────\n",
        "dataset_ENZ = TUDataset(\n",
        "    root='/tmp/ENZYMES',\n",
        "    name='ENZYMES',\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "# According to the paper: ENZYMES has 600 graphs; split 70/10/20 = 420/60/120\n",
        "train_ENZ = dataset_ENZ[:420]    # first 420 graphs\n",
        "val_ENZ   = dataset_ENZ[420:480] # next 60 graphs\n",
        "test_ENZ  = dataset_ENZ[480:]    # last 120 graphs\n",
        "\n",
        "print(f\"ENZ: total={len(dataset_ENZ)} | train={len(train_ENZ)} | val={len(val_ENZ)} | test={len(test_ENZ)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BS_ewo_O6t1",
        "outputId": "95a6ea9d-1e1d-4b65-a946-8ef4462569d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZ: total=600 | train=420 | val=60 | test=120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 1.2.2 PROTEINS ─────────────────────────────────────────────────────────\n",
        "dataset_PRO = TUDataset(\n",
        "    root='/tmp/PROTEINS',\n",
        "    name='PROTEINS',\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "# PROTEINS has 1113 graphs; 70%≈779, 10%≈111, 20%≈223\n",
        "train_PRO = dataset_PRO[:779]\n",
        "val_PRO   = dataset_PRO[779: 779+111]\n",
        "test_PRO  = dataset_PRO[779+111:]\n",
        "\n",
        "print(f\"PRO: total={len(dataset_PRO)} | train={len(train_PRO)} | val={len(val_PRO)} | test={len(test_PRO)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRcR76_0O-Qk",
        "outputId": "2d3415c0-04ed-41dc-a922-87ecdd2b82c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRO: total=1113 | train=779 | val=111 | test=223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 1.3 GCNMean ────────────────────────────────────────────────────────────\n",
        "class GCNMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)   # [batch_size, hidden_channels]\n",
        "        return self.lin(x)               # [batch_size, num_classes]\n",
        "\n",
        "\n",
        "# ── 1.3 GCNDiff ────────────────────────────────────────────────────────────\n",
        "class GCNDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        # Note: “Diff” variant often means we add a skip connection after each layer.\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x  # initial input\n",
        "        x1 = F.relu(self.conv1(x0, edge_index)) + x0   # residual\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        x2 = F.relu(self.conv2(x1, edge_index)) + x1   # residual\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        x3 = F.relu(self.conv3(x2, edge_index)) + x2   # residual\n",
        "        x3 = global_mean_pool(x3, batch)\n",
        "        return self.lin(x3)\n",
        "\n",
        "\n",
        "# ── 1.3 SAGEMean ───────────────────────────────────────────────────────────\n",
        "class SAGEMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "# ── 1.3 SAGEDiff ───────────────────────────────────────────────────────────\n",
        "class SAGEDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        x1 = F.relu(self.conv1(x0, edge_index)) + x0\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        x2 = F.relu(self.conv2(x1, edge_index)) + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        x3 = F.relu(self.conv3(x2, edge_index)) + x2\n",
        "        x3 = global_mean_pool(x3, batch)\n",
        "        return self.lin(x3)\n"
      ],
      "metadata": {
        "id": "zDMS_fhCPBSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_graph_cls_victim(\n",
        "    model: nn.Module,\n",
        "    train_dataset: TUDataset,\n",
        "    val_dataset: TUDataset,\n",
        "    test_dataset: TUDataset,\n",
        "    model_name: str,\n",
        "    num_epochs: int = 200,\n",
        "    batch_size: int = 32,\n",
        "    lr: float = 0.01,\n",
        "    weight_decay: float = 5e-4\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains `model` on train_dataset, monitors val_dataset, and returns test accuracy.\n",
        "    Saves the best‐validation checkpoint as \"{model_name}.pth\".\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state   = None\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0.0; correct = 0; total = 0\n",
        "        for batch in DataLoader(train_dataset, batch_size=batch_size, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)  # logits [batch_size, num_classes]\n",
        "            labels = batch.y.view(-1)\n",
        "            loss = loss_fn(out, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += batch.num_graphs\n",
        "\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct_val = 0; total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in DataLoader(val_dataset, batch_size=batch_size, shuffle=False):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "                correct_val += (out == batch.y.view(-1)).sum().item()\n",
        "                total_val += batch.num_graphs\n",
        "        val_acc = correct_val / total_val\n",
        "\n",
        "        # Save best‐val checkpoint\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state   = model.state_dict()\n",
        "\n",
        "        if epoch % 20 == 0 or epoch == 1 or epoch == num_epochs:\n",
        "            print(f\"[{model_name}] Epoch {epoch:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Load best‐val before computing final test accuracy\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # Test accuracy\n",
        "    model.eval()\n",
        "    correct_test = 0; total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(test_dataset, batch_size=batch_size, shuffle=False):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "            correct_test += (out == batch.y.view(-1)).sum().item()\n",
        "            total_test += batch.num_graphs\n",
        "    test_acc = correct_test / total_test\n",
        "\n",
        "    # Save final best‐val weights\n",
        "    torch.save(model.state_dict(), f\"victim_{model_name}.pth\")\n",
        "    print(f\"[{model_name}] Best val acc {best_val_acc:.4f} → saved to victim_{model_name}.pth | Test Acc {test_acc:.4f}\")\n",
        "\n",
        "    return test_acc\n"
      ],
      "metadata": {
        "id": "We8PkJ5zPMIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Load ENZYMES (70∶10∶20 split)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "dataset_ENZ = TUDataset(\n",
        "    root='/tmp/ENZYMES',\n",
        "    name='ENZYMES',\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "train_ENZ = dataset_ENZ[:420]    # 420 graphs\n",
        "val_ENZ   = dataset_ENZ[420:480] # 60 graphs\n",
        "test_ENZ  = dataset_ENZ[480:]    # 120 graphs\n",
        "\n",
        "print(f\"ENZYMES: total={len(dataset_ENZ)} | train={len(train_ENZ)} | val={len(val_ENZ)} | test={len(test_ENZ)}\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Define GCNMean, GCNDiff, SAGEMean, SAGEDiff (with corrected residuals)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# ── 2.1 GCNMean ─────────────────────────────────────────────────────────────\n",
        "class GCNMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "# ── 2.2 Corrected GCNDiff ─────────────────────────────────────────────────────\n",
        "class GCNDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # If in_channels != hidden_channels, project input x0 → hidden_channels before adding\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        # Project x0 if needed\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        # Layer 1 + residual\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))      # [num_nodes, hidden_channels]\n",
        "        x1 = h1 + x0_proj                             # project‐skip + conv output\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Layer 2 + residual\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))       # [num_nodes, hidden_channels]\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Layer 3 + residual\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)             # [batch_size, hidden_channels]\n",
        "        return self.lin(out)                          # [batch_size, num_classes]\n",
        "\n",
        "\n",
        "# ── 2.3 SAGEMean ─────────────────────────────────────────────────────────────\n",
        "class SAGEMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "# ── 2.4 Corrected SAGEDiff ────────────────────────────────────────────────────\n",
        "class SAGEDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        # Project inputs if needed\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        # Layer 1 + residual\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))       # [num_nodes, hidden_channels]\n",
        "        x1 = h1 + x0_proj\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Layer 2 + residual\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))       # [num_nodes, hidden_channels]\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Layer 3 + residual\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)              # [batch_size, hidden_channels]\n",
        "        return self.lin(out)                           # [batch_size, num_classes]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Train all four victims on ENZYMES\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 3.1 GCNMean\n",
        "model_name = \"ENZ_GCNMean\"\n",
        "model = GCNMean(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ")\n",
        "test_acc_ENZ_GCNMean = train_graph_cls_victim(\n",
        "    model=model,\n",
        "    train_dataset=train_ENZ,\n",
        "    val_dataset=val_ENZ,\n",
        "    test_dataset=test_ENZ,\n",
        "    model_name=model_name\n",
        ")\n",
        "\n",
        "# 3.2 GCNDiff\n",
        "model_name = \"ENZ_GCNDiff\"\n",
        "model = GCNDiff(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ")\n",
        "test_acc_ENZ_GCNDiff = train_graph_cls_victim(\n",
        "    model=model,\n",
        "    train_dataset=train_ENZ,\n",
        "    val_dataset=val_ENZ,\n",
        "    test_dataset=test_ENZ,\n",
        "    model_name=model_name\n",
        ")\n",
        "\n",
        "# 3.3 SAGEMean\n",
        "model_name = \"ENZ_SAGEMean\"\n",
        "model = SAGEMean(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ")\n",
        "test_acc_ENZ_SAGEMean = train_graph_cls_victim(\n",
        "    model=model,\n",
        "    train_dataset=train_ENZ,\n",
        "    val_dataset=val_ENZ,\n",
        "    test_dataset=test_ENZ,\n",
        "    model_name=model_name\n",
        ")\n",
        "\n",
        "# 3.4 SAGEDiff\n",
        "model_name = \"ENZ_SAGEDiff\"\n",
        "model = SAGEDiff(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ")\n",
        "test_acc_ENZ_SAGEDiff = train_graph_cls_victim(\n",
        "    model=model,\n",
        "    train_dataset=train_ENZ,\n",
        "    val_dataset=val_ENZ,\n",
        "    test_dataset=test_ENZ,\n",
        "    model_name=model_name\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlH4JgEbPPGM",
        "outputId": "50873c94-8c22-4187-db37-60d70a9a056a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES: total=600 | train=420 | val=60 | test=120\n",
            "[ENZ_GCNMean] Epoch 001 | Train Acc: 0.1738 | Val Acc: 0.2667\n",
            "[ENZ_GCNMean] Epoch 020 | Train Acc: 0.2500 | Val Acc: 0.2000\n",
            "[ENZ_GCNMean] Epoch 040 | Train Acc: 0.3000 | Val Acc: 0.2833\n",
            "[ENZ_GCNMean] Epoch 060 | Train Acc: 0.3381 | Val Acc: 0.2833\n",
            "[ENZ_GCNMean] Epoch 080 | Train Acc: 0.3905 | Val Acc: 0.3500\n",
            "[ENZ_GCNMean] Epoch 100 | Train Acc: 0.3786 | Val Acc: 0.2667\n",
            "[ENZ_GCNMean] Epoch 120 | Train Acc: 0.3929 | Val Acc: 0.3167\n",
            "[ENZ_GCNMean] Epoch 140 | Train Acc: 0.3881 | Val Acc: 0.2667\n",
            "[ENZ_GCNMean] Epoch 160 | Train Acc: 0.4190 | Val Acc: 0.2833\n",
            "[ENZ_GCNMean] Epoch 180 | Train Acc: 0.4333 | Val Acc: 0.2667\n",
            "[ENZ_GCNMean] Epoch 200 | Train Acc: 0.4714 | Val Acc: 0.2333\n",
            "[ENZ_GCNMean] Best val acc 0.3500 → saved to victim_ENZ_GCNMean.pth | Test Acc 0.4000\n",
            "[ENZ_GCNDiff] Epoch 001 | Train Acc: 0.1738 | Val Acc: 0.2667\n",
            "[ENZ_GCNDiff] Epoch 020 | Train Acc: 0.3381 | Val Acc: 0.2333\n",
            "[ENZ_GCNDiff] Epoch 040 | Train Acc: 0.3762 | Val Acc: 0.2167\n",
            "[ENZ_GCNDiff] Epoch 060 | Train Acc: 0.3786 | Val Acc: 0.3333\n",
            "[ENZ_GCNDiff] Epoch 080 | Train Acc: 0.4143 | Val Acc: 0.3000\n",
            "[ENZ_GCNDiff] Epoch 100 | Train Acc: 0.4548 | Val Acc: 0.3333\n",
            "[ENZ_GCNDiff] Epoch 120 | Train Acc: 0.4190 | Val Acc: 0.2667\n",
            "[ENZ_GCNDiff] Epoch 140 | Train Acc: 0.4595 | Val Acc: 0.2667\n",
            "[ENZ_GCNDiff] Epoch 160 | Train Acc: 0.4333 | Val Acc: 0.2833\n",
            "[ENZ_GCNDiff] Epoch 180 | Train Acc: 0.4595 | Val Acc: 0.2667\n",
            "[ENZ_GCNDiff] Epoch 200 | Train Acc: 0.4952 | Val Acc: 0.3500\n",
            "[ENZ_GCNDiff] Best val acc 0.3833 → saved to victim_ENZ_GCNDiff.pth | Test Acc 0.2917\n",
            "[ENZ_SAGEMean] Epoch 001 | Train Acc: 0.1643 | Val Acc: 0.1833\n",
            "[ENZ_SAGEMean] Epoch 020 | Train Acc: 0.2929 | Val Acc: 0.2500\n",
            "[ENZ_SAGEMean] Epoch 040 | Train Acc: 0.3357 | Val Acc: 0.2667\n",
            "[ENZ_SAGEMean] Epoch 060 | Train Acc: 0.3619 | Val Acc: 0.3167\n",
            "[ENZ_SAGEMean] Epoch 080 | Train Acc: 0.3810 | Val Acc: 0.3000\n",
            "[ENZ_SAGEMean] Epoch 100 | Train Acc: 0.4143 | Val Acc: 0.3167\n",
            "[ENZ_SAGEMean] Epoch 120 | Train Acc: 0.4286 | Val Acc: 0.4333\n",
            "[ENZ_SAGEMean] Epoch 140 | Train Acc: 0.4548 | Val Acc: 0.3333\n",
            "[ENZ_SAGEMean] Epoch 160 | Train Acc: 0.4286 | Val Acc: 0.3667\n",
            "[ENZ_SAGEMean] Epoch 180 | Train Acc: 0.4738 | Val Acc: 0.3167\n",
            "[ENZ_SAGEMean] Epoch 200 | Train Acc: 0.4333 | Val Acc: 0.3500\n",
            "[ENZ_SAGEMean] Best val acc 0.4333 → saved to victim_ENZ_SAGEMean.pth | Test Acc 0.4083\n",
            "[ENZ_SAGEDiff] Epoch 001 | Train Acc: 0.1810 | Val Acc: 0.2000\n",
            "[ENZ_SAGEDiff] Epoch 020 | Train Acc: 0.3524 | Val Acc: 0.3000\n",
            "[ENZ_SAGEDiff] Epoch 040 | Train Acc: 0.3643 | Val Acc: 0.3167\n",
            "[ENZ_SAGEDiff] Epoch 060 | Train Acc: 0.4262 | Val Acc: 0.3000\n",
            "[ENZ_SAGEDiff] Epoch 080 | Train Acc: 0.4500 | Val Acc: 0.3000\n",
            "[ENZ_SAGEDiff] Epoch 100 | Train Acc: 0.4810 | Val Acc: 0.3167\n",
            "[ENZ_SAGEDiff] Epoch 120 | Train Acc: 0.4929 | Val Acc: 0.3333\n",
            "[ENZ_SAGEDiff] Epoch 140 | Train Acc: 0.4952 | Val Acc: 0.2833\n",
            "[ENZ_SAGEDiff] Epoch 160 | Train Acc: 0.5286 | Val Acc: 0.3667\n",
            "[ENZ_SAGEDiff] Epoch 180 | Train Acc: 0.5167 | Val Acc: 0.3333\n",
            "[ENZ_SAGEDiff] Epoch 200 | Train Acc: 0.4810 | Val Acc: 0.2167\n",
            "[ENZ_SAGEDiff] Best val acc 0.4333 → saved to victim_ENZ_SAGEDiff.pth | Test Acc 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/gnnfingers/checkpoints\n",
        "!mkdir -p /content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/positive\n",
        "!mkdir -p /content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/negative\n"
      ],
      "metadata": {
        "id": "JfkG90xltrP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the victim models\n",
        "!cp /content/victim_ENZ_GCNMean.pth   /content/drive/MyDrive/gnnfingers/checkpoints/\n",
        "!cp /content/victim_ENZ_GCNDiff.pth   /content/drive/MyDrive/gnnfingers/checkpoints/\n",
        "!cp /content/victim_ENZ_SAGEMean.pth  /content/drive/MyDrive/gnnfingers/checkpoints/\n",
        "!cp /content/victim_ENZ_SAGEDiff.pth /content/drive/MyDrive/gnnfingers/checkpoints/\n",
        "\n",
        "# Copy the positive & negative variants you generated\n",
        "!cp -r /content/variants/ENZ_GCNMean/positive   /content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/\n",
        "!cp -r /content/variants/ENZ_GCNMean/negative   /content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/\n"
      ],
      "metadata": {
        "id": "ZxQqpYzgtsyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the checkpoint folder\n",
        "!ls /content/drive/MyDrive/gnnfingers/checkpoints\n",
        "\n",
        "# List the variants folder\n",
        "!ls /content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/positive | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DlEHfiDsr1k",
        "outputId": "d1c7981d-3c9c-43a3-98db-404012eb98a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "victim_ENZ_GCNDiff.pth\tvictim_ENZ_SAGEDiff.pth\n",
            "victim_ENZ_GCNMean.pth\tvictim_ENZ_SAGEMean.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 0) Make sure these top-level folders exist (they’ll be relative to your notebook)\n",
        "os.makedirs(\"ENZYMES\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_GCNMean/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_GCNMean/negative\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_GCNDiff/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_GCNDiff/negative\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_SAGEMean/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_SAGEMean/negative\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_SAGEDiff/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/ENZ_SAGEDiff/negative\", exist_ok=True)\n",
        "\n",
        "# 1) Load ENZYMES without an absolute path—this creates a folder \"./ENZYMES\"\n",
        "dataset_ENZ = TUDataset(root=\"ENZYMES\",\n",
        "                        name=\"ENZYMES\",\n",
        "                        transform=OneHotDegree(max_degree=10)).shuffle()\n",
        "\n",
        "train_ENZ = dataset_ENZ[:420]\n",
        "val_ENZ   = dataset_ENZ[420:480]\n",
        "test_ENZ  = dataset_ENZ[480:]\n",
        "\n",
        "print(f\"ENZYMES: total={len(dataset_ENZ)} | train={len(train_ENZ)} | val={len(val_ENZ)} | test={len(test_ENZ)}\")\n",
        "\n",
        "# 2) Define architectures (GCNMean, corrected GCNDiff, SAGEMean, corrected SAGEDiff)\n",
        "\n",
        "class GCNMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "class GCNDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))\n",
        "        x1 = h1 + x0_proj\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)\n",
        "        return self.lin(out)\n",
        "\n",
        "class SAGEMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "class SAGEDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))\n",
        "        x1 = h1 + x0_proj\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)\n",
        "        return self.lin(out)\n",
        "\n",
        "# 3) Helper to train & save victims (again, saves to \"./checkpoints/victim_<model_name>.pth\")\n",
        "def train_graph_cls_victim(model, train_dataset, val_dataset, test_dataset, model_name,\n",
        "                           num_epochs=200, batch_size=32, lr=0.01, weight_decay=5e-4):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    best_val_acc, best_state = 0.0, None\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        model.train()\n",
        "        total_correct, total = 0, 0\n",
        "        for batch in DataLoader(train_dataset, batch_size=batch_size, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = out.argmax(dim=1)\n",
        "            total_correct += (preds == batch.y.view(-1)).sum().item()\n",
        "            total += batch.num_graphs\n",
        "\n",
        "        train_acc = total_correct / total\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_correct, val_total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in DataLoader(val_dataset, batch_size=batch_size, shuffle=False):\n",
        "                batch = batch.to(device)\n",
        "                preds = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "                val_correct += (preds == batch.y.view(-1)).sum().item()\n",
        "                val_total += batch.num_graphs\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc, best_state = val_acc, model.state_dict()\n",
        "\n",
        "        if epoch % 20 == 0 or epoch == 1 or epoch == num_epochs:\n",
        "            print(f\"[{model_name}] Epoch {epoch:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    test_correct, test_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(test_dataset, batch_size=batch_size, shuffle=False):\n",
        "            batch = batch.to(device)\n",
        "            preds = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "            test_correct += (preds == batch.y.view(-1)).sum().item()\n",
        "            test_total += batch.num_graphs\n",
        "    test_acc = test_correct / test_total\n",
        "\n",
        "    torch.save(model.state_dict(), f\"checkpoints/victim_{model_name}.pth\")\n",
        "    print(f\"[{model_name}] Best val acc {best_val_acc:.4f} → saved to checkpoints/victim_{model_name}.pth | Test Acc {test_acc:.4f}\")\n",
        "    return test_acc\n",
        "\n",
        "# 4) Train each ENZ “victim” without any absolute paths\n",
        "test_acc_ENZ_GCNMean = train_graph_cls_victim(\n",
        "    GCNMean(dataset_ENZ.num_node_features, 64, dataset_ENZ.num_classes, dropout=0.5),\n",
        "    train_ENZ, val_ENZ, test_ENZ, \"ENZ_GCNMean\"\n",
        ")\n",
        "\n",
        "test_acc_ENZ_GCNDiff = train_graph_cls_victim(\n",
        "    GCNDiff(dataset_ENZ.num_node_features, 64, dataset_ENZ.num_classes, dropout=0.5),\n",
        "    train_ENZ, val_ENZ, test_ENZ, \"ENZ_GCNDiff\"\n",
        ")\n",
        "\n",
        "test_acc_ENZ_SAGEMean = train_graph_cls_victim(\n",
        "    SAGEMean(dataset_ENZ.num_node_features, 64, dataset_ENZ.num_classes, dropout=0.5),\n",
        "    train_ENZ, val_ENZ, test_ENZ, \"ENZ_SAGEMean\"\n",
        ")\n",
        "\n",
        "test_acc_ENZ_SAGEDiff = train_graph_cls_victim(\n",
        "    SAGEDiff(dataset_ENZ.num_node_features, 64, dataset_ENZ.num_classes, dropout=0.5),\n",
        "    train_ENZ, val_ENZ, test_ENZ, \"ENZ_SAGEDiff\"\n",
        ")\n",
        "\n",
        "# 5) Now generate positive/negative variants for ENZ_GCNMean, using relative paths:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# Positive variants for ENZ_GCNMean\n",
        "for vid in range(200):\n",
        "    model = GCNMean(dataset_ENZ.num_node_features, 64, dataset_ENZ.num_classes, dropout=0.5).to(device)\n",
        "    model.load_state_dict(torch.load(\"checkpoints/victim_ENZ_GCNMean.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    strat = vid // 50\n",
        "    if strat == 0:\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"lin\" not in name:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 1:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 2:\n",
        "        # Distill GCNMean → SAGEMean\n",
        "        class SAGEMean(nn.Module):\n",
        "            def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "                super().__init__()\n",
        "                self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "                self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "                self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "                self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "                self.dropout = dropout\n",
        "            def forward(self, x, edge_index, batch):\n",
        "                x = F.relu(self.conv1(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv2(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv3(x, edge_index))\n",
        "                x = global_mean_pool(x, batch)\n",
        "                return self.lin(x)\n",
        "\n",
        "        teacher = model\n",
        "        student = SAGEMean(dataset_ENZ.num_node_features, 64, dataset_ENZ.num_classes, dropout=0.5).to(device)\n",
        "        optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "        temperature = 5.0\n",
        "        for epoch in range(1, 11):\n",
        "            student.train()\n",
        "            for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                with torch.no_grad():\n",
        "                    t_logits = teacher(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                s_logits = student(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                t_probs = F.log_softmax(t_logits, dim=1)\n",
        "                s_probs = F.log_softmax(s_logits, dim=1)\n",
        "                loss = F.kl_div(s_probs, t_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    else:\n",
        "        all_weights = []\n",
        "        for param in model.parameters():\n",
        "            if param.requires_grad:\n",
        "                all_weights.append(param.view(-1).cpu().abs())\n",
        "        all_weights = torch.cat(all_weights)\n",
        "        threshold = torch.quantile(all_weights, 0.10)\n",
        "        with torch.no_grad():\n",
        "            for param in model.parameters():\n",
        "                mask = param.abs() < threshold.to(param.device)\n",
        "                param[mask] = 0.0\n",
        "\n",
        "    torch.save(model.state_dict(), f\"variants/ENZ_GCNMean/positive/positive_{vid:03d}.pth\")\n",
        "\n",
        "print(\"Done 200 positive variants for ENZ_GCNMean.\")\n",
        "\n",
        "# Negative variants for ENZ_GCNMean\n",
        "for vid in range(200):\n",
        "    model = GCNMean(dataset_ENZ.num_node_features, 64, dataset_ENZ.num_classes, dropout=0.5).to(device)\n",
        "    torch.manual_seed(vid)\n",
        "    random.seed(vid)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, 201):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    torch.save(model.state_dict(), f\"variants/ENZ_GCNMean/negative/negative_{vid:03d}.pth\")\n",
        "\n",
        "print(\"Done 200 negative variants for ENZ_GCNMean.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "Rd_EnjOaPXtt",
        "outputId": "045577c8-3424-4c2d-db6e-e96583431c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES: total=600 | train=420 | val=60 | test=120\n",
            "[ENZ_GCNMean] Epoch 001 | Train Acc: 0.1262 | Val Acc: 0.1167\n",
            "[ENZ_GCNMean] Epoch 020 | Train Acc: 0.2667 | Val Acc: 0.2500\n",
            "[ENZ_GCNMean] Epoch 040 | Train Acc: 0.3405 | Val Acc: 0.3000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-337e7dbe9adc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# 4) Train each ENZ “victim” without any absolute paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m test_acc_ENZ_GCNMean = train_graph_cls_victim(\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0mGCNMean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ENZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_node_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ENZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mtrain_ENZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ENZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ENZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ENZ_GCNMean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-337e7dbe9adc>\u001b[0m in \u001b[0;36mtrain_graph_cls_victim\u001b[0;34m(model, train_dataset, val_dataset, test_dataset, model_name, num_epochs, batch_size, lr, weight_decay)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-337e7dbe9adc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_mean_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/pool/glob.py\u001b[0m in \u001b[0;36mglobal_mean_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(src, ref, dim)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch.utils.data import TensorDataset, DataLoader as TorchDL\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (A) Reload ENZYMES, victim, and negative variants\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 1) Load ENZ (relative to \"./ENZYMES\" folder)\n",
        "dataset_ENZ = TUDataset(\n",
        "    root=\"ENZYMES\",\n",
        "    name=\"ENZYMES\",\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "train_ENZ = dataset_ENZ[:420]\n",
        "val_ENZ   = dataset_ENZ[420:480]\n",
        "test_ENZ  = dataset_ENZ[480:]\n",
        "\n",
        "# 2) Define the same GCNMean as before\n",
        "class GCNMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "# 3) Reload victim_ENZ_GCNMean\n",
        "victim_path = os.path.join(\"checkpoints\", \"victim_ENZ_GCNMean.pth\")\n",
        "victim_ENZ_GCNMean = GCNMean(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "victim_ENZ_GCNMean.load_state_dict(torch.load(victim_path = os.path.join(\"checkpoints\", \"victim_ENZ_GCNMean.pth\")\n",
        ", map_location=device))\n",
        "victim_ENZ_GCNMean.eval()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (B) Define and train U_ENZ_GCNMean (FingerprintNetMLP) – Block 4\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class FingerprintNetMLP_ENZ(nn.Module):\n",
        "    def __init__(self, embed_dim=64, key_dim=64, hidden1=256, hidden2=128, hidden3=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.key = nn.Parameter(torch.randn(key_dim), requires_grad=False)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim + key_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden2, hidden3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden3, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, emb_batch):\n",
        "        key_rep = self.key.unsqueeze(0).expand(emb_batch.size(0), -1).to(emb_batch.device)\n",
        "        x = torch.cat([emb_batch, key_rep], dim=1)  # [batch_size, 128]\n",
        "        return self.net(x)                          # [batch_size, 2]\n",
        "\n",
        "U_ENZ_GCNMean = FingerprintNetMLP_ENZ(\n",
        "    embed_dim=64,\n",
        "    key_dim=64,\n",
        "    hidden1=256,\n",
        "    hidden2=128,\n",
        "    hidden3=64,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "# 4) Collect 100 “victim” embeddings (label=1) from train_ENZ[:100]\n",
        "victim_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "        batch = batch.to(device)\n",
        "        # We need the 64‐dim “pooled” output from the last conv:\n",
        "        # Modify GCNMean to also return its pooled vector if needed; here we recompute:\n",
        "        x1 = F.relu(victim_ENZ_GCNMean.conv1(batch.x, batch.edge_index))\n",
        "        x1 = F.dropout(x1, p=victim_ENZ_GCNMean.dropout, training=False)\n",
        "        x2 = F.relu(victim_ENZ_GCNMean.conv2(x1, batch.edge_index))\n",
        "        x2 = F.dropout(x2, p=victim_ENZ_GCNMean.dropout, training=False)\n",
        "        x3 = F.relu(victim_ENZ_GCNMean.conv3(x2, batch.edge_index))\n",
        "        pooled = global_mean_pool(x3, batch.batch)  # [batch_size, 64]\n",
        "        for emb in pooled:\n",
        "            victim_embeddings.append((emb.cpu(), 1))\n",
        "assert len(victim_embeddings) == 100\n",
        "\n",
        "# 5) Collect 100 negative embeddings from 20 negative variants (5 each)\n",
        "negative_embeddings = []\n",
        "num_neg_models = 20\n",
        "embs_per_model  = 100 // num_neg_models  # = 5\n",
        "\n",
        "for neg_i in range(num_neg_models):\n",
        "    neg_model = GCNMean(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_GCNMean\", \"negative\", f\"negative_{neg_i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "\n",
        "    collected = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "            batch = batch.to(device)\n",
        "            x1 = F.relu(neg_model.conv1(batch.x, batch.edge_index))\n",
        "            x1 = F.dropout(x1, p=neg_model.dropout, training=False)\n",
        "            x2 = F.relu(neg_model.conv2(x1, batch.edge_index))\n",
        "            x2 = F.dropout(x2, p=neg_model.dropout, training=False)\n",
        "            x3 = F.relu(neg_model.conv3(x2, batch.edge_index))\n",
        "            pooled_neg = global_mean_pool(x3, batch.batch)  # [batch_size, 64]\n",
        "            for emb in pooled_neg:\n",
        "                if collected >= embs_per_model:\n",
        "                    break\n",
        "                negative_embeddings.append((emb.cpu(), 0))\n",
        "                collected += 1\n",
        "            if collected >= embs_per_model:\n",
        "                break\n",
        "\n",
        "assert len(negative_embeddings) == 100\n",
        "\n",
        "# 6) Combine, shuffle, and build U’s training loader\n",
        "fp_pairs = victim_embeddings + negative_embeddings\n",
        "random.shuffle(fp_pairs)\n",
        "\n",
        "features = torch.stack([pair[0] for pair in fp_pairs], dim=0)  # [200, 64]\n",
        "labels   = torch.tensor([pair[1] for pair in fp_pairs], dtype=torch.long)  # [200]\n",
        "\n",
        "fp_dataset = TensorDataset(features, labels)\n",
        "fp_loader  = TorchDL(fp_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"U’s training set ready: features [200,64], labels [200]\")\n",
        "\n",
        "# Train U_ENZ_GCNMean\n",
        "opt_U = torch.optim.Adam(U_ENZ_GCNMean.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "loss_fn_fp = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    U_ENZ_GCNMean.train()\n",
        "    total_loss = 0.0\n",
        "    correct_fp = 0\n",
        "    total_fp = 0\n",
        "\n",
        "    for (emb_batch, lbl_batch) in fp_loader:\n",
        "        emb_batch = emb_batch.to(device)\n",
        "        lbl_batch = lbl_batch.to(device)\n",
        "\n",
        "        logits = U_ENZ_GCNMean(emb_batch)     # [batch_size, 2]\n",
        "        loss = loss_fn_fp(logits, lbl_batch)\n",
        "\n",
        "        opt_U.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_U.step()\n",
        "\n",
        "        total_loss += loss.item() * emb_batch.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_fp += (preds == lbl_batch).sum().item()\n",
        "        total_fp += emb_batch.size(0)\n",
        "\n",
        "    train_loss = total_loss / total_fp\n",
        "    train_acc  = correct_fp / total_fp\n",
        "    print(f\"[U_ENZ_GCNMean] Epoch {epoch:03d} | U-loss: {train_loss:.4f} | U-acc: {train_acc:.4f}\")\n",
        "\n",
        "    if train_acc >= 0.95:\n",
        "        print(f\"→ Reached 95% accuracy at epoch {epoch}, stopping early.\")\n",
        "        break\n",
        "\n",
        "U_ENZ_GCNMean.eval()\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "torch.save(U_ENZ_GCNMean.state_dict(), \"checkpoints/U_ENZ_GCNMean.pth\")\n",
        "print(\"Finished training U_ENZ_GCNMean; saved to checkpoints/U_ENZ_GCNMean.pth\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (C) Helper to extract a single model’s fingerprint\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def extract_fingerprint_graph_cls(model: nn.Module, U_net: nn.Module, loader: DataLoader) -> np.ndarray:\n",
        "    model.eval()\n",
        "    U_net.eval()\n",
        "    pooled_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            x1 = F.relu(model.conv1(batch.x, batch.edge_index))\n",
        "            x1 = F.dropout(x1, p=model.dropout, training=False)\n",
        "            x2 = F.relu(model.conv2(x1, batch.edge_index))\n",
        "            x2 = F.dropout(x2, p=model.dropout, training=False)\n",
        "            x3 = F.relu(model.conv3(x2, batch.edge_index))\n",
        "            pooled = global_mean_pool(x3, batch.batch)  # [batch_size, 64]\n",
        "            pooled_list.append(pooled.cpu())\n",
        "\n",
        "    all_pooled = torch.cat(pooled_list, dim=0)  # [Q, 64]\n",
        "    with torch.no_grad():\n",
        "        key_rep = U_net.key.unsqueeze(0).expand(all_pooled.size(0), -1).to(device)\n",
        "        x = torch.cat([all_pooled.to(device), key_rep], dim=1)  # [Q, 128]\n",
        "        features = U_net.net[:-1](x)  # [Q, hidden3=64]\n",
        "    return features.detach().mean(dim=0).cpu().numpy()  # [64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (D) Extract fp_victim (Q=100)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# Build query_loader from test_ENZ[:100]\n",
        "query_graphs = list(test_ENZ[:100])\n",
        "query_loader = DataLoader(query_graphs, batch_size=32, shuffle=False)\n",
        "\n",
        "fp_victim_ENZ = extract_fingerprint_graph_cls(victim_ENZ_GCNMean, U_ENZ_GCNMean, query_loader)  # [64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (E) Extract fp_negatives (200 × 64)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_neg_ENZ = []\n",
        "for i in range(200):\n",
        "    neg_model = GCNMean(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_GCNMean\", \"negative\", f\"negative_{i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "\n",
        "    fp_neg_ENZ.append(extract_fingerprint_graph_cls(neg_model, U_ENZ_GCNMean, query_loader))\n",
        "\n",
        "fp_neg_ENZ = np.stack(fp_neg_ENZ, axis=0)  # [200, 64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (F) Extract fp_retrained_victims (R = 5, 10 epochs)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def train_graph_cls_repl_ENZ(seed: int, num_epochs: int = 10):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    model = GCNMean(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            labels = batch.y.view(-1)\n",
        "            loss = loss_fn(out, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "fp_rt_ENZ_list = []\n",
        "for seed in range(5):\n",
        "    rt_model = train_graph_cls_repl_ENZ(seed + 100, num_epochs=10)\n",
        "    fp_rt_ENZ_list.append(extract_fingerprint_graph_cls(rt_model, U_ENZ_GCNMean, query_loader))\n",
        "\n",
        "fp_rt_ENZ = np.stack(fp_rt_ENZ_list, axis=0)  # [5, 64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (G) Compute cosine‐similarity scores: robustness (R) and uniqueness (K)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_v2 = fp_victim_ENZ.reshape(1, -1)  # [1, 64]\n",
        "robustness_ENZ = cosine_similarity(fp_v2, fp_rt_ENZ)[0]   # shape [5]\n",
        "uniqueness_ENZ = cosine_similarity(fp_v2, fp_neg_ENZ)[0]  # shape [200]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (H) Build ARUC: sweep τ ∈ [-1, 1]\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "thresholds = np.linspace(-1.0, 1.0, 100)\n",
        "TPR = []\n",
        "FPR = []\n",
        "for τ in thresholds:\n",
        "    TPR.append(np.mean(robustness_ENZ >= τ))\n",
        "    FPR.append(np.mean(uniqueness_ENZ >= τ))\n",
        "TPR = np.array(TPR)\n",
        "FPR = np.array(FPR)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (I) Plot and save ARUC\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(FPR, TPR, linewidth=2)\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ARUC: ENZ GCNMean (R=5, K=200, Q=100)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ARUC_ENZ_GCNMean_Q100.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "KBqb4tzXPbFU",
        "outputId": "625c3a57-931f-4053-965a-753ea019bb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a6899fb4ed7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTUDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotDegree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch.utils.data import TensorDataset, DataLoader as TorchDL\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (A) Reload ENZYMES, GCNDiff victim, and negative variants\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "dataset_ENZ = TUDataset(\n",
        "    root=\"ENZYMES\",\n",
        "    name=\"ENZYMES\",\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "train_ENZ = dataset_ENZ[:420]\n",
        "val_ENZ   = dataset_ENZ[420:480]\n",
        "test_ENZ  = dataset_ENZ[480:]\n",
        "\n",
        "# 2) Corrected GCNDiff definition (same as you used when training)\n",
        "class GCNDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))\n",
        "        x1 = h1 + x0_proj\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)\n",
        "        return self.lin(out)\n",
        "\n",
        "# 3) Reload victim_ENZ_GCNDiff\n",
        "victim_path = os.path.join(\"checkpoints\", \"victim_ENZ_GCNDiff.pth\")\n",
        "victim_ENZ_GCNDiff = GCNDiff(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "victim_ENZ_GCNDiff.load_state_dict(torch.load(victim_path, map_location=device))\n",
        "victim_ENZ_GCNDiff.eval()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (B) Define & train U_ENZ_GCNDiff\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class FingerprintNetMLP_ENZ(nn.Module):\n",
        "    def __init__(self, embed_dim=64, key_dim=64, hidden1=256, hidden2=128, hidden3=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.key = nn.Parameter(torch.randn(key_dim), requires_grad=False)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim + key_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden2, hidden3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden3, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, emb_batch):\n",
        "        key_rep = self.key.unsqueeze(0).expand(emb_batch.size(0), -1).to(emb_batch.device)\n",
        "        x = torch.cat([emb_batch, key_rep], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "U_ENZ_GCNDiff = FingerprintNetMLP_ENZ().to(device)\n",
        "\n",
        "# 4) Collect 100 “victim” embeddings with label=1 (train_ENZ[:100])\n",
        "victim_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "        batch = batch.to(device)\n",
        "        # Manually extract pooled 64‐d vector:\n",
        "        h1 = F.relu(victim_ENZ_GCNDiff.conv1(batch.x, batch.edge_index))\n",
        "        h1 = F.dropout(h1, p=victim_ENZ_GCNDiff.dropout, training=False)\n",
        "        h2 = F.relu(victim_ENZ_GCNDiff.conv2(h1, batch.edge_index))\n",
        "        h2 = F.dropout(h2, p=victim_ENZ_GCNDiff.dropout, training=False)\n",
        "        h3 = F.relu(victim_ENZ_GCNDiff.conv3(h2, batch.edge_index))\n",
        "        pooled = global_mean_pool(h3, batch.batch)  # [batch_size,64]\n",
        "        for emb in pooled:\n",
        "            victim_embeddings.append((emb.cpu(), 1))\n",
        "assert len(victim_embeddings) == 100\n",
        "\n",
        "# 5) Collect 100 negative embeddings from 20 negative models (5 per model)\n",
        "negative_embeddings = []\n",
        "num_neg_models = 20\n",
        "embs_per_model  = 100 // num_neg_models  # 5\n",
        "\n",
        "for neg_i in range(num_neg_models):\n",
        "    neg_model = GCNDiff(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_GCNDiff\", \"negative\", f\"negative_{neg_i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "\n",
        "    collected = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "            batch = batch.to(device)\n",
        "            h1 = F.relu(neg_model.conv1(batch.x, batch.edge_index))\n",
        "            h1 = F.dropout(h1, p=neg_model.dropout, training=False)\n",
        "            h2 = F.relu(neg_model.conv2(h1, batch.edge_index))\n",
        "            h2 = F.dropout(h2, p=neg_model.dropout, training=False)\n",
        "            h3 = F.relu(neg_model.conv3(h2, batch.edge_index))\n",
        "            pooled_neg = global_mean_pool(h3, batch.batch)  # [batch_size,64]\n",
        "            for emb in pooled_neg:\n",
        "                if collected >= embs_per_model:\n",
        "                    break\n",
        "                negative_embeddings.append((emb.cpu(), 0))\n",
        "                collected += 1\n",
        "            if collected >= embs_per_model:\n",
        "                break\n",
        "\n",
        "assert len(negative_embeddings) == 100\n",
        "\n",
        "# 6) Combine, shuffle, and build U’s training DataLoader\n",
        "fp_pairs = victim_embeddings + negative_embeddings\n",
        "random.shuffle(fp_pairs)\n",
        "\n",
        "features = torch.stack([p[0] for p in fp_pairs], dim=0)  # [200,64]\n",
        "labels   = torch.tensor([p[1] for p in fp_pairs], dtype=torch.long)  # [200]\n",
        "\n",
        "fp_dataset = TensorDataset(features, labels)\n",
        "fp_loader  = TorchDL(fp_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Train U_ENZ_GCNDiff\n",
        "opt_U = torch.optim.Adam(U_ENZ_GCNDiff.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "loss_fn_fp = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    U_ENZ_GCNDiff.train()\n",
        "    total_loss = 0.0; correct_fp = 0; total_fp = 0\n",
        "    for emb_batch, lbl_batch in fp_loader:\n",
        "        emb_batch = emb_batch.to(device)\n",
        "        lbl_batch = lbl_batch.to(device)\n",
        "        logits = U_ENZ_GCNDiff(emb_batch)\n",
        "        loss = loss_fn_fp(logits, lbl_batch)\n",
        "        opt_U.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_U.step()\n",
        "        total_loss += loss.item() * emb_batch.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_fp += (preds == lbl_batch).sum().item()\n",
        "        total_fp += emb_batch.size(0)\n",
        "\n",
        "    train_acc = correct_fp / total_fp\n",
        "    print(f\"[U_ENZ_GCNDiff] Epoch {epoch:03d} | U-loss: {total_loss/total_fp:.4f} | U-acc: {train_acc:.4f}\")\n",
        "    if train_acc >= 0.95:\n",
        "        print(f\"→ Reached 95% at epoch {epoch}, stopping\")\n",
        "        break\n",
        "\n",
        "U_ENZ_GCNDiff.eval()\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "torch.save(U_ENZ_GCNDiff.state_dict(), \"checkpoints/U_ENZ_GCNDiff.pth\")\n",
        "print(\"Saved U_ENZ_GCNDiff to checkpoints/U_ENZ_GCNDiff.pth\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (C) Helper: extract graph‐cls fingerprint using U\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def extract_fingerprint_graph_cls(model: nn.Module, U_net: nn.Module, loader: DataLoader) -> np.ndarray:\n",
        "    model.eval(); U_net.eval()\n",
        "    pooled_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            h1 = F.relu(model.conv1(batch.x, batch.edge_index))\n",
        "            h1 = F.dropout(h1, p=model.dropout, training=False)\n",
        "            h2 = F.relu(model.conv2(h1, batch.edge_index))\n",
        "            h2 = F.dropout(h2, p=model.dropout, training=False)\n",
        "            h3 = F.relu(model.conv3(h2, batch.edge_index))\n",
        "            pooled = global_mean_pool(h3, batch.batch)  # [batch_size,64]\n",
        "            pooled_list.append(pooled.cpu())\n",
        "    all_pooled = torch.cat(pooled_list, dim=0)  # [Q,64]\n",
        "    with torch.no_grad():\n",
        "        key_rep = U_net.key.unsqueeze(0).expand(all_pooled.size(0), -1).to(device)\n",
        "        x = torch.cat([all_pooled.to(device), key_rep], dim=1)  # [Q,128]\n",
        "        features = U_net.net[:-1](x)  # [Q,64]\n",
        "    return features.detach().mean(dim=0).cpu().numpy()  # [64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (D) Extract fp_victim (Q=100)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "query_graphs = list(test_ENZ[:100])\n",
        "query_loader = DataLoader(query_graphs, batch_size=32, shuffle=False)\n",
        "\n",
        "fp_victim_ENZ = extract_fingerprint_graph_cls(victim_ENZ_GCNDiff, U_ENZ_GCNDiff, query_loader)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (E) Extract fp_negatives (200×64)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_neg_ENZ = []\n",
        "for i in range(200):\n",
        "    neg_model = GCNDiff(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_GCNDiff\", \"negative\", f\"negative_{i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "    fp_neg_ENZ.append(extract_fingerprint_graph_cls(neg_model, U_ENZ_GCNDiff, query_loader))\n",
        "\n",
        "fp_neg_ENZ = np.stack(fp_neg_ENZ, axis=0)  # [200,64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (F) Extract fp_retrained_victims (R=5, 10 epochs)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def train_graph_cls_repl_ENZ_diff(seed: int, num_epochs: int = 10):\n",
        "    torch.manual_seed(seed); random.seed(seed)\n",
        "    model = GCNDiff(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "fp_rt_ENZ_list = []\n",
        "for seed in range(5):\n",
        "    rt_model = train_graph_cls_repl_ENZ_diff(seed + 100, num_epochs=10)\n",
        "    fp_rt_ENZ_list.append(extract_fingerprint_graph_cls(rt_model, U_ENZ_GCNDiff, query_loader))\n",
        "\n",
        "fp_rt_ENZ = np.stack(fp_rt_ENZ_list, axis=0)  # [5,64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (G) Compute cosine‐similarity: robustness & uniqueness\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_v2 = fp_victim_ENZ.reshape(1, -1)                     # [1,64]\n",
        "robustness_ENZ = cosine_similarity(fp_v2, fp_rt_ENZ)[0]   # [5]\n",
        "uniqueness_ENZ = cosine_similarity(fp_v2, fp_neg_ENZ)[0]  # [200]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (H) Build ARUC: thresholds ∈ [−1,1]\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "thresholds = np.linspace(-1.0, 1.0, 100)\n",
        "TPR = []; FPR = []\n",
        "for τ in thresholds:\n",
        "    TPR.append(np.mean(robustness_ENZ >= τ))\n",
        "    FPR.append(np.mean(uniqueness_ENZ >= τ))\n",
        "TPR = np.array(TPR); FPR = np.array(FPR)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (I) Plot & save ARUC\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(FPR, TPR, linewidth=2)\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ARUC: ENZ GCNDiff (R=5, K=200, Q=100)\")\n",
        "plt.grid(True); plt.tight_layout()\n",
        "plt.savefig(\"ARUC_ENZ_GCNDiff_Q100.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uPhbuL_1PfIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
        "from torch.utils.data import TensorDataset, DataLoader as TorchDL\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (A) Reload ENZYMES, SAGEMean victim, and negative variants\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "dataset_ENZ = TUDataset(\n",
        "    root=\"ENZYMES\",\n",
        "    name=\"ENZYMES\",\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "train_ENZ = dataset_ENZ[:420]\n",
        "val_ENZ   = dataset_ENZ[420:480]\n",
        "test_ENZ  = dataset_ENZ[480:]\n",
        "\n",
        "# 2) SAGEMean definition\n",
        "class SAGEMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "# 3) Reload victim_ENZ_SAGEMean\n",
        "victim_path = os.path.join(\"checkpoints\", \"victim_ENZ_SAGEMean.pth\")\n",
        "victim_ENZ_SAGEMean = SAGEMean(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "victim_ENZ_SAGEMean.load_state_dict(torch.load(victim_path, map_location=device))\n",
        "victim_ENZ_SAGEMean.eval()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (B) Define & train U_ENZ_SAGEMean\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class FingerprintNetMLP_ENZ(nn.Module):\n",
        "    def __init__(self, embed_dim=64, key_dim=64, hidden1=256, hidden2=128, hidden3=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.key = nn.Parameter(torch.randn(key_dim), requires_grad=False)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim + key_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden2, hidden3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden3, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, emb_batch):\n",
        "        key_rep = self.key.unsqueeze(0).expand(emb_batch.size(0), -1).to(emb_batch.device)\n",
        "        x = torch.cat([emb_batch, key_rep], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "U_ENZ_SAGEMean = FingerprintNetMLP_ENZ().to(device)\n",
        "\n",
        "# 4) Collect 100 victim embeddings (train_ENZ[:100])\n",
        "victim_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "        batch = batch.to(device)\n",
        "        h1 = F.relu(victim_ENZ_SAGEMean.conv1(batch.x, batch.edge_index))\n",
        "        h1 = F.dropout(h1, p=victim_ENZ_SAGEMean.dropout, training=False)\n",
        "        h2 = F.relu(victim_ENZ_SAGEMean.conv2(h1, batch.edge_index))\n",
        "        h2 = F.dropout(h2, p=victim_ENZ_SAGEMean.dropout, training=False)\n",
        "        h3 = F.relu(victim_ENZ_SAGEMean.conv3(h2, batch.edge_index))\n",
        "        pooled = global_mean_pool(h3, batch.batch)\n",
        "        for emb in pooled:\n",
        "            victim_embeddings.append((emb.cpu(), 1))\n",
        "assert len(victim_embeddings) == 100\n",
        "\n",
        "# 5) Collect 100 negative embeddings from 20 negative models\n",
        "negative_embeddings = []\n",
        "num_neg_models = 20\n",
        "embs_per_model  = 100 // num_neg_models  # 5\n",
        "\n",
        "for neg_i in range(num_neg_models):\n",
        "    neg_model = SAGEMean(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_SAGEMean\", \"negative\", f\"negative_{neg_i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "\n",
        "    collected = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "            batch = batch.to(device)\n",
        "            h1 = F.relu(neg_model.conv1(batch.x, batch.edge_index))\n",
        "            h1 = F.dropout(h1, p=neg_model.dropout, training=False)\n",
        "            h2 = F.relu(neg_model.conv2(h1, batch.edge_index))\n",
        "            h2 = F.dropout(h2, p=neg_model.dropout, training=False)\n",
        "            h3 = F.relu(neg_model.conv3(h2, batch.edge_index))\n",
        "            pooled_neg = global_mean_pool(h3, batch.batch)\n",
        "            for emb in pooled_neg:\n",
        "                if collected >= embs_per_model:\n",
        "                    break\n",
        "                negative_embeddings.append((emb.cpu(), 0))\n",
        "                collected += 1\n",
        "            if collected >= embs_per_model:\n",
        "                break\n",
        "\n",
        "assert len(negative_embeddings) == 100\n",
        "\n",
        "# 6) Combine, shuffle, and build U’s DataLoader\n",
        "fp_pairs = victim_embeddings + negative_embeddings\n",
        "random.shuffle(fp_pairs)\n",
        "\n",
        "features = torch.stack([p[0] for p in fp_pairs], dim=0)  # [200,64]\n",
        "labels   = torch.tensor([p[1] for p in fp_pairs], dtype=torch.long)  # [200]\n",
        "\n",
        "fp_dataset = TensorDataset(features, labels)\n",
        "fp_loader  = TorchDL(fp_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "opt_U = torch.optim.Adam(U_ENZ_SAGEMean.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "loss_fn_fp = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    U_ENZ_SAGEMean.train()\n",
        "    total_loss = 0.0; correct_fp = 0; total_fp = 0\n",
        "    for emb_batch, lbl_batch in fp_loader:\n",
        "        emb_batch = emb_batch.to(device)\n",
        "        lbl_batch = lbl_batch.to(device)\n",
        "        logits = U_ENZ_SAGEMean(emb_batch)\n",
        "        loss = loss_fn_fp(logits, lbl_batch)\n",
        "        opt_U.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_U.step()\n",
        "        total_loss += loss.item() * emb_batch.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_fp += (preds == lbl_batch).sum().item()\n",
        "        total_fp += emb_batch.size(0)\n",
        "\n",
        "    train_acc = correct_fp / total_fp\n",
        "    print(f\"[U_ENZ_SAGEMean] Epoch {epoch:03d} | U-loss: {total_loss/total_fp:.4f} | U-acc: {train_acc:.4f}\")\n",
        "    if train_acc >= 0.95:\n",
        "        print(f\"→ Reached 95% at epoch {epoch}, stopping\")\n",
        "        break\n",
        "\n",
        "U_ENZ_SAGEMean.eval()\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "torch.save(U_ENZ_SAGEMean.state_dict(), \"checkpoints/U_ENZ_SAGEMean.pth\")\n",
        "print(\"Saved U_ENZ_SAGEMean to checkpoints/U_ENZ_SAGEMean.pth\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (C) Helper: extract fingerprint\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def extract_fingerprint_graph_cls(model: nn.Module, U_net: nn.Module, loader: DataLoader) -> np.ndarray:\n",
        "    model.eval(); U_net.eval()\n",
        "    pooled_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            h1 = F.relu(model.conv1(batch.x, batch.edge_index))\n",
        "            h1 = F.dropout(h1, p=model.dropout, training=False)\n",
        "            h2 = F.relu(model.conv2(h1, batch.edge_index))\n",
        "            h2 = F.dropout(h2, p=model.dropout, training=False)\n",
        "            h3 = F.relu(model.conv3(h2, batch.edge_index))\n",
        "            pooled = global_mean_pool(h3, batch.batch)\n",
        "            pooled_list.append(pooled.cpu())\n",
        "    all_pooled = torch.cat(pooled_list, dim=0)  # [Q,64]\n",
        "    with torch.no_grad():\n",
        "        key_rep = U_net.key.unsqueeze(0).expand(all_pooled.size(0), -1).to(device)\n",
        "        x = torch.cat([all_pooled.to(device), key_rep], dim=1)  # [Q,128]\n",
        "        features = U_net.net[:-1](x)  # [Q,64]\n",
        "    return features.detach().mean(dim=0).cpu().numpy()  # [64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (D) Extract fp_victim (Q=100)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "query_graphs = list(test_ENZ[:100])\n",
        "query_loader = DataLoader(query_graphs, batch_size=32, shuffle=False)\n",
        "\n",
        "fp_victim_ENZ = extract_fingerprint_graph_cls(victim_ENZ_SAGEMean, U_ENZ_SAGEMean, query_loader)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (E) Extract fp_negatives (200×64)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_neg_ENZ = []\n",
        "for i in range(200):\n",
        "    neg_model = SAGEMean(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_SAGEMean\", \"negative\", f\"negative_{i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "    fp_neg_ENZ.append(extract_fingerprint_graph_cls(neg_model, U_ENZ_SAGEMean, query_loader))\n",
        "\n",
        "fp_neg_ENZ = np.stack(fp_neg_ENZ, axis=0)  # [200,64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (F) Extract fp_retrained_victims (R=5, 10 epochs)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def train_graph_cls_repl_ENZ_sage(seed: int, num_epochs: int = 10):\n",
        "    torch.manual_seed(seed); random.seed(seed)\n",
        "    model = SAGEMean(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "fp_rt_ENZ_list = []\n",
        "for seed in range(5):\n",
        "    rt_model = train_graph_cls_repl_ENZ_sage(seed + 100, num_epochs=10)\n",
        "    fp_rt_ENZ_list.append(extract_fingerprint_graph_cls(rt_model, U_ENZ_SAGEMean, query_loader))\n",
        "\n",
        "fp_rt_ENZ = np.stack(fp_rt_ENZ_list, axis=0)  # [5,64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (G) Compute cosine‐similarity: robustness & uniqueness\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_v2 = fp_victim_ENZ.reshape(1, -1)\n",
        "robustness_ENZ = cosine_similarity(fp_v2, fp_rt_ENZ)[0]   # [5]\n",
        "uniqueness_ENZ = cosine_similarity(fp_v2, fp_neg_ENZ)[0]  # [200]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (H) Build ARUC: thresholds ∈ [−1,1]\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "thresholds = np.linspace(-1.0, 1.0, 100)\n",
        "TPR = []; FPR = []\n",
        "for τ in thresholds:\n",
        "    TPR.append(np.mean(robustness_ENZ >= τ))\n",
        "    FPR.append(np.mean(uniqueness_ENZ >= τ))\n",
        "TPR = np.array(TPR); FPR = np.array(FPR)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (I) Plot & save ARUC\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(FPR, TPR, linewidth=2)\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ARUC: ENZ SAGEMean (R=5, K=200, Q=100)\")\n",
        "plt.grid(True); plt.tight_layout()\n",
        "plt.savefig(\"ARUC_ENZ_SAGEMean_Q100.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2Av9ePe-Ph0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
        "from torch.utils.data import TensorDataset, DataLoader as TorchDL\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (A) Reload ENZYMES, SAGEDiff victim, and negative variants\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "dataset_ENZ = TUDataset(\n",
        "    root=\"ENZYMES\",\n",
        "    name=\"ENZYMES\",\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "train_ENZ = dataset_ENZ[:420]\n",
        "val_ENZ   = dataset_ENZ[420:480]\n",
        "test_ENZ  = dataset_ENZ[480:]\n",
        "\n",
        "# 2) Corrected SAGEDiff definition\n",
        "class SAGEDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))\n",
        "        x1 = h1 + x0_proj\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)\n",
        "        return self.lin(out)\n",
        "\n",
        "# 3) Reload victim_ENZ_SAGEDiff\n",
        "victim_path = os.path.join(\"checkpoints\", \"victim_ENZ_SAGEDiff.pth\")\n",
        "victim_ENZ_SAGEDiff = SAGEDiff(\n",
        "    in_channels=dataset_ENZ.num_node_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset_ENZ.num_classes,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "victim_ENZ_SAGEDiff.load_state_dict(torch.load(victim_path, map_location=device))\n",
        "victim_ENZ_SAGEDiff.eval()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (B) Define & train U_ENZ_SAGEDiff\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class FingerprintNetMLP_ENZ(nn.Module):\n",
        "    def __init__(self, embed_dim=64, key_dim=64, hidden1=256, hidden2=128, hidden3=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.key = nn.Parameter(torch.randn(key_dim), requires_grad=False)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim + key_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden2, hidden3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden3, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, emb_batch):\n",
        "        key_rep = self.key.unsqueeze(0).expand(emb_batch.size(0), -1).to(emb_batch.device)\n",
        "        x = torch.cat([emb_batch, key_rep], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "U_ENZ_SAGEDiff = FingerprintNetMLP_ENZ().to(device)\n",
        "\n",
        "# 4) Collect 100 victim embeddings (train_ENZ[:100])\n",
        "victim_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "        batch = batch.to(device)\n",
        "        h1 = F.relu(victim_ENZ_SAGEDiff.conv1(batch.x, batch.edge_index))\n",
        "        h1 = F.dropout(h1, p=victim_ENZ_SAGEDiff.dropout, training=False)\n",
        "        h2 = F.relu(victim_ENZ_SAGEDiff.conv2(h1, batch.edge_index))\n",
        "        h2 = F.dropout(h2, p=victim_ENZ_SAGEDiff.dropout, training=False)\n",
        "        h3 = F.relu(victim_ENZ_SAGEDiff.conv3(h2, batch.edge_index))\n",
        "        pooled = global_mean_pool(h3, batch.batch)\n",
        "        for emb in pooled:\n",
        "            victim_embeddings.append((emb.cpu(), 1))\n",
        "assert len(victim_embeddings) == 100\n",
        "\n",
        "# 5) Collect 100 negative embeddings (20 models × 5 each)\n",
        "negative_embeddings = []\n",
        "num_neg_models = 20\n",
        "embs_per_model  = 100 // num_neg_models  # 5\n",
        "\n",
        "for neg_i in range(num_neg_models):\n",
        "    neg_model = SAGEDiff(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_SAGEDiff\", \"negative\", f\"negative_{neg_i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "\n",
        "    collected = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(train_ENZ[:100], batch_size=32, shuffle=False):\n",
        "            batch = batch.to(device)\n",
        "            h1 = F.relu(neg_model.conv1(batch.x, batch.edge_index))\n",
        "            h1 = F.dropout(h1, p=neg_model.dropout, training=False)\n",
        "            h2 = F.relu(neg_model.conv2(h1, batch.edge_index))\n",
        "            h2 = F.dropout(h2, p=neg_model.dropout, training=False)\n",
        "            h3 = F.relu(neg_model.conv3(h2, batch.edge_index))\n",
        "            pooled_neg = global_mean_pool(h3, batch.batch)\n",
        "            for emb in pooled_neg:\n",
        "                if collected >= embs_per_model:\n",
        "                    break\n",
        "                negative_embeddings.append((emb.cpu(), 0))\n",
        "                collected += 1\n",
        "            if collected >= embs_per_model:\n",
        "                break\n",
        "\n",
        "assert len(negative_embeddings) == 100\n",
        "\n",
        "# 6) Combine, shuffle, and build U’s DataLoader\n",
        "fp_pairs = victim_embeddings + negative_embeddings\n",
        "random.shuffle(fp_pairs)\n",
        "\n",
        "features = torch.stack([p[0] for p in fp_pairs], dim=0)  # [200,64]\n",
        "labels   = torch.tensor([p[1] for p in fp_pairs], dtype=torch.long)  # [200]\n",
        "\n",
        "fp_dataset = TensorDataset(features, labels)\n",
        "fp_loader  = TorchDL(fp_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "opt_U = torch.optim.Adam(U_ENZ_SAGEDiff.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "loss_fn_fp = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    U_ENZ_SAGEDiff.train()\n",
        "    total_loss = 0.0; correct_fp = 0; total_fp = 0\n",
        "    for emb_batch, lbl_batch in fp_loader:\n",
        "        emb_batch = emb_batch.to(device)\n",
        "        lbl_batch = lbl_batch.to(device)\n",
        "        logits = U_ENZ_SAGEDiff(emb_batch)\n",
        "        loss = loss_fn_fp(logits, lbl_batch)\n",
        "        opt_U.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_U.step()\n",
        "        total_loss += loss.item() * emb_batch.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct_fp += (preds == lbl_batch).sum().item()\n",
        "        total_fp += emb_batch.size(0)\n",
        "\n",
        "    train_acc = correct_fp / total_fp\n",
        "    print(f\"[U_ENZ_SAGEDiff] Epoch {epoch:03d} | U-loss: {total_loss/total_fp:.4f} | U-acc: {train_acc:.4f}\")\n",
        "    if train_acc >= 0.95:\n",
        "        print(f\"→ Reached 95% at epoch {epoch}, stopping\")\n",
        "        break\n",
        "\n",
        "U_ENZ_SAGEDiff.eval()\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "torch.save(U_ENZ_SAGEDiff.state_dict(), \"checkpoints/U_ENZ_SAGEDiff.pth\")\n",
        "print(\"Saved U_ENZ_SAGEDiff to checkpoints/U_ENZ_SAGEDiff.pth\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (C) Helper: extract fingerprint\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def extract_fingerprint_graph_cls(model: nn.Module, U_net: nn.Module, loader: DataLoader) -> np.ndarray:\n",
        "    model.eval(); U_net.eval()\n",
        "    pooled_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            h1 = F.relu(model.conv1(batch.x, batch.edge_index))\n",
        "            h1 = F.dropout(h1, p=model.dropout, training=False)\n",
        "            h2 = F.relu(model.conv2(h1, batch.edge_index))\n",
        "            h2 = F.dropout(h2, p=model.dropout, training=False)\n",
        "            h3 = F.relu(model.conv3(h2, batch.edge_index))\n",
        "            pooled = global_mean_pool(h3, batch.batch)\n",
        "            pooled_list.append(pooled.cpu())\n",
        "    all_pooled = torch.cat(pooled_list, dim=0)  # [Q,64]\n",
        "    with torch.no_grad():\n",
        "        key_rep = U_net.key.unsqueeze(0).expand(all_pooled.size(0), -1).to(device)\n",
        "        x = torch.cat([all_pooled.to(device), key_rep], dim=1)  # [Q,128]\n",
        "        features = U_net.net[:-1](x)  # [Q,64]\n",
        "    return features.detach().mean(dim=0).cpu().numpy()  # [64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (D) Extract fp_victim (Q=100)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "query_graphs = list(test_ENZ[:100])\n",
        "query_loader = DataLoader(query_graphs, batch_size=32, shuffle=False)\n",
        "\n",
        "fp_victim_ENZ = extract_fingerprint_graph_cls(victim_ENZ_SAGEDiff, U_ENZ_SAGEDiff, query_loader)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (E) Extract fp_negatives (200×64)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_neg_ENZ = []\n",
        "for i in range(200):\n",
        "    neg_model = SAGEDiff(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    neg_path = os.path.join(\"variants\", \"ENZ_SAGEDiff\", \"negative\", f\"negative_{i:03d}.pth\")\n",
        "    neg_model.load_state_dict(torch.load(neg_path, map_location=device))\n",
        "    neg_model.eval()\n",
        "    fp_neg_ENZ.append(extract_fingerprint_graph_cls(neg_model, U_ENZ_SAGEDiff, query_loader))\n",
        "\n",
        "fp_neg_ENZ = np.stack(fp_neg_ENZ, axis=0)  # [200,64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (F) Extract fp_retrained_victims (R=5, 10 epochs)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def train_graph_cls_repl_ENZ_sage_diff(seed: int, num_epochs: int = 10):\n",
        "    torch.manual_seed(seed); random.seed(seed)\n",
        "    model = SAGEDiff(\n",
        "        in_channels=dataset_ENZ.num_node_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset_ENZ.num_classes,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_ENZ, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "fp_rt_ENZ_list = []\n",
        "for seed in range(5):\n",
        "    rt_model = train_graph_cls_repl_ENZ_sage_diff(seed + 100, num_epochs=10)\n",
        "    fp_rt_ENZ_list.append(extract_fingerprint_graph_cls(rt_model, U_ENZ_SAGEDiff, query_loader))\n",
        "\n",
        "fp_rt_ENZ = np.stack(fp_rt_ENZ_list, axis=0)  # [5,64]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (G) Compute cosine‐similarity: robustness & uniqueness\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fp_v2 = fp_victim_ENZ.reshape(1, -1)\n",
        "robustness_ENZ = cosine_similarity(fp_v2, fp_rt_ENZ)[0]   # [5]\n",
        "uniqueness_ENZ = cosine_similarity(fp_v2, fp_neg_ENZ)[0]  # [200]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (H) Build ARUC: thresholds ∈ [−1,1]\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "thresholds = np.linspace(-1.0, 1.0, 100)\n",
        "TPR = []; FPR = []\n",
        "for τ in thresholds:\n",
        "    TPR.append(np.mean(robustness_ENZ >= τ))\n",
        "    FPR.append(np.mean(uniqueness_ENZ >= τ))\n",
        "TPR = np.array(TPR); FPR = np.array(FPR)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# (I) Plot & save ARUC\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(FPR, TPR, linewidth=2)\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ARUC: ENZ SAGEDiff (R=5, K=200, Q=100)\")\n",
        "plt.grid(True); plt.tight_layout()\n",
        "plt.savefig(\"ARUC_ENZ_SAGEDiff_Q100.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h2v48jZVPmHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Install PyG (only run once per notebook; comment it out afterwards)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python3 -c \"import torch; print(torch.__version__.split('+')[0] )\")+cu$(nvidia-smi | grep -Po '(?<=CUDA Version: )\\d+\\.\\d+')/download.html --quiet\n",
        "!pip install torch-sparse   -f https://data.pyg.org/whl/torch-$(python3 -c \"import torch; print(torch.__version__.split('+')[0] )\")+cu$(nvidia-smi | grep -Po '(?<=CUDA Version: )\\d+\\.\\d+')/download.html --quiet\n",
        "!pip install torch-cluster  -f https://data.pyg.org/whl/torch-$(python3 -c \"import torch; print(torch.__version__.split('+')[0] )\")+cu$(nvidia-smi | grep -Po '(?<=CUDA Version: )\\d+\\.\\d+')/download.html --quiet\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-$(python3 -c \"import torch; print(torch.__version__.split('+')[0] )\")+cu$(nvidia-smi | grep -Po '(?<=CUDA Version: )\\d+\\.\\d+')/download.html --quiet\n",
        "!pip install torch-geometric --quiet\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Load & split NCI1\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create local folders if needed\n",
        "os.makedirs(\"NCI1\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_GCNMean/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_GCNMean/negative\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_GCNDiff/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_GCNDiff/negative\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_SAGEMean/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_SAGEMean/negative\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_SAGEDiff/positive\", exist_ok=True)\n",
        "os.makedirs(\"variants/NCI1_SAGEDiff/negative\", exist_ok=True)\n",
        "\n",
        "# Download and split (70∶10∶20)\n",
        "dataset_NCI1 = TUDataset(\n",
        "    root=\"NCI1\",\n",
        "    name=\"NCI1\",\n",
        "    transform=OneHotDegree(max_degree=10)\n",
        ").shuffle()\n",
        "\n",
        "n = len(dataset_NCI1)\n",
        "n_train = int(0.70 * n)\n",
        "n_val   = int(0.10 * n)\n",
        "train_NCI1 = dataset_NCI1[:n_train]\n",
        "val_NCI1   = dataset_NCI1[n_train : n_train + n_val]\n",
        "test_NCI1  = dataset_NCI1[n_train + n_val : ]\n",
        "\n",
        "print(f\"NCI1: total={n}   | train={len(train_NCI1)}   | val={len(val_NCI1)}   | test={len(test_NCI1)}\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2) Define four victim architectures\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 2.1 GCNMean\n",
        "class GCNMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "# 2.2 GCNDiff (with projection skip)\n",
        "class GCNDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))\n",
        "        x1 = h1 + x0_proj\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)\n",
        "        return self.lin(out)\n",
        "\n",
        "# 2.3 SAGEMean\n",
        "class SAGEMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "# 2.4 SAGEDiff (with projection skip)\n",
        "class SAGEDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "        self.dropout = dropout\n",
        "        if in_channels != hidden_channels:\n",
        "            self.skip_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        else:\n",
        "            self.skip_lin = None\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x0 = x\n",
        "        x0_proj = self.skip_lin(x0) if (self.skip_lin is not None) else x0\n",
        "\n",
        "        h1 = F.relu(self.conv1(x0, edge_index))\n",
        "        x1 = h1 + x0_proj\n",
        "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "\n",
        "        h2 = F.relu(self.conv2(x1, edge_index))\n",
        "        x2 = h2 + x1\n",
        "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        h3 = F.relu(self.conv3(x2, edge_index))\n",
        "        x3 = h3 + x2\n",
        "\n",
        "        out = global_mean_pool(x3, batch)\n",
        "        return self.lin(out)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3) Training helper (same as ENZymES)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def train_graph_cls_victim(\n",
        "    model: nn.Module,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    test_dataset,\n",
        "    model_name: str,\n",
        "    num_epochs: int = 200,\n",
        "    batch_size: int = 32,\n",
        "    lr: float = 0.01,\n",
        "    weight_decay: float = 5e-4\n",
        "):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state   = None\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        correct = total = 0\n",
        "        for batch in DataLoader(train_dataset, batch_size=batch_size, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            lbl = batch.y.view(-1)\n",
        "            loss = loss_fn(out, lbl)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == lbl).sum().item()\n",
        "            total += batch.num_graphs\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct_val = total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in DataLoader(val_dataset, batch_size=batch_size, shuffle=False):\n",
        "                batch = batch.to(device)\n",
        "                preds = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "                correct_val += (preds == batch.y.view(-1)).sum().item()\n",
        "                total_val += batch.num_graphs\n",
        "        val_acc = correct_val / total_val\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state   = model.state_dict()\n",
        "\n",
        "        if epoch % 20 == 0 or epoch == 1 or epoch == num_epochs:\n",
        "            print(f\"[{model_name}] Epoch {epoch:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Load best‐val state, then test\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    model.eval()\n",
        "    correct_test = total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in DataLoader(test_dataset, batch_size=batch_size, shuffle=False):\n",
        "            batch = batch.to(device)\n",
        "            preds = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "            correct_test += (preds == batch.y.view(-1)).sum().item()\n",
        "            total_test += batch.num_graphs\n",
        "    test_acc = correct_test / total_test\n",
        "\n",
        "    ckpt_path = os.path.join(\"checkpoints\", f\"victim_NCI1_{model_name}.pth\")\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"[{model_name}] Best val acc {best_val_acc:.4f} → saved to {ckpt_path} | Test Acc {test_acc:.4f}\")\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4) Train all four victims on NCI1\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 4.1 GCNMean\n",
        "test_acc_NCI1_GCNMean = train_graph_cls_victim(\n",
        "    GCNMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5),\n",
        "    train_NCI1, val_NCI1, test_NCI1,\n",
        "    \"GCNMean\"\n",
        ")\n",
        "\n",
        "# 4.2 GCNDiff\n",
        "test_acc_NCI1_GCNDiff = train_graph_cls_victim(\n",
        "    GCNDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5),\n",
        "    train_NCI1, val_NCI1, test_NCI1,\n",
        "    \"GCNDiff\"\n",
        ")\n",
        "\n",
        "# 4.3 SAGEMean\n",
        "test_acc_NCI1_SAGEMean = train_graph_cls_victim(\n",
        "    SAGEMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5),\n",
        "    train_NCI1, val_NCI1, test_NCI1,\n",
        "    \"SAGEMean\"\n",
        ")\n",
        "\n",
        "# 4.4 SAGEDiff\n",
        "test_acc_NCI1_SAGEDiff = train_graph_cls_victim(\n",
        "    SAGEDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5),\n",
        "    train_NCI1, val_NCI1, test_NCI1,\n",
        "    \"SAGEDiff\"\n",
        ")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 5) Generate 200 positive & 200 negative variants per architecture\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# (a) NCI1_GCNMean\n",
        "for vid in range(200):\n",
        "    model = GCNMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    model.load_state_dict(torch.load(\"checkpoints/victim_NCI1_GCNMean.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    strat = vid // 50\n",
        "    # A) fine‐tune last linear only (10 epochs)\n",
        "    if strat == 0:\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"lin\" not in name:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    # B) fine‐tune all layers (10 epochs)\n",
        "    elif strat == 1:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    # C) distill GCNMean → SAGEMean (10 epochs)\n",
        "    elif strat == 2:\n",
        "        # SAGEMean “student”\n",
        "        class SAGEMean_Student(nn.Module):\n",
        "            def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "                super().__init__()\n",
        "                self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "                self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "                self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "                self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "                self.dropout = dropout\n",
        "\n",
        "            def forward(self, x, edge_index, batch):\n",
        "                x = F.relu(self.conv1(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv2(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv3(x, edge_index))\n",
        "                x = global_mean_pool(x, batch)\n",
        "                return self.lin(x)\n",
        "\n",
        "        teacher = model\n",
        "        student = SAGEMean_Student(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "        optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "        temperature = 5.0\n",
        "        for epoch in range(1, 11):\n",
        "            student.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                with torch.no_grad():\n",
        "                    t_logits = teacher(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                s_logits = student(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                t_probs = F.log_softmax(t_logits, dim=1)\n",
        "                s_probs = F.log_softmax(s_logits, dim=1)\n",
        "                loss = F.kl_div(s_probs, t_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    # D) prune 10% weights\n",
        "    else:\n",
        "        all_w = []\n",
        "        for p in model.parameters():\n",
        "            if p.requires_grad:\n",
        "                all_w.append(p.view(-1).abs().cpu())\n",
        "        all_w = torch.cat(all_w)\n",
        "        thr = torch.quantile(all_w, 0.10)\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                mask = p.abs() < thr.to(p.device)\n",
        "                p[mask] = 0.0\n",
        "\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_GCNMean/positive/positive_{vid:03d}.pth\")\n",
        "\n",
        "# Negative variants for NCI1_GCNMean\n",
        "for vid in range(200):\n",
        "    model = GCNMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    torch.manual_seed(vid); random.seed(vid)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, 201):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_GCNMean/negative/negative_{vid:03d}.pth\")\n",
        "\n",
        "\n",
        "# (b) NCI1_GCNDiff\n",
        "for vid in range(200):\n",
        "    model = GCNDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    model.load_state_dict(torch.load(\"checkpoints/victim_NCI1_GCNDiff.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    strat = vid // 50\n",
        "    if strat == 0:\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"lin\" not in name:\n",
        "                p.requires_grad = False\n",
        "            else:\n",
        "                p.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 1:\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 2:\n",
        "        # Distill → SAGEMean\n",
        "        class SAGEMean_Student(nn.Module):\n",
        "            def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "                super().__init__()\n",
        "                self.conv1  = SAGEConv(in_channels, hidden_channels)\n",
        "                self.conv2  = SAGEConv(hidden_channels, hidden_channels)\n",
        "                self.conv3  = SAGEConv(hidden_channels, hidden_channels)\n",
        "                self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "                self.dropout = dropout\n",
        "            def forward(self, x, edge_index, batch):\n",
        "                x = F.relu(self.conv1(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv2(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv3(x, edge_index))\n",
        "                x = global_mean_pool(x, batch)\n",
        "                return self.lin(x)\n",
        "\n",
        "        teacher = model\n",
        "        student = SAGEMean_Student(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "        optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "        temperature = 5.0\n",
        "        for epoch in range(1, 11):\n",
        "            student.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                with torch.no_grad():\n",
        "                    t_logits = teacher(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                s_logits = student(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                t_probs = F.log_softmax(t_logits, dim=1)\n",
        "                s_probs = F.log_softmax(s_logits, dim=1)\n",
        "                loss = F.kl_div(s_probs, t_probs, reduction=\"batchmean\") * (temperature**2)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    else:\n",
        "        all_w = []\n",
        "        for p in model.parameters():\n",
        "            if p.requires_grad:\n",
        "                all_w.append(p.view(-1).abs().cpu())\n",
        "        all_w = torch.cat(all_w)\n",
        "        thr = torch.quantile(all_w, 0.10)\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                mask = p.abs() < thr.to(p.device)\n",
        "                p[mask] = 0.0\n",
        "\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_GCNDiff/positive/positive_{vid:03d}.pth\")\n",
        "\n",
        "for vid in range(200):\n",
        "    model = GCNDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    torch.manual_seed(vid); random.seed(vid)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, 201):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_GCNDiff/negative/negative_{vid:03d}.pth\")\n",
        "\n",
        "\n",
        "# (c) NCI1_SAGEMean\n",
        "for vid in range(200):\n",
        "    model = SAGEMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    model.load_state_dict(torch.load(\"checkpoints/victim_NCI1_SAGEMean.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    strat = vid // 50\n",
        "    if strat == 0:\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"lin\" not in name:\n",
        "                p.requires_grad = False\n",
        "            else:\n",
        "                p.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 1:\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 2:\n",
        "        # Distill → GCNMean\n",
        "        class GCNMean_Student(nn.Module):\n",
        "            def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "                super().__init__()\n",
        "                self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "                self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "                self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "                self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "                self.dropout = dropout\n",
        "\n",
        "            def forward(self, x, edge_index, batch):\n",
        "                x = F.relu(self.conv1(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv2(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv3(x, edge_index))\n",
        "                x = global_mean_pool(x, batch)\n",
        "                return self.lin(x)\n",
        "\n",
        "        teacher = model\n",
        "        student = GCNMean_Student(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "        optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "        temperature = 5.0\n",
        "        for epoch in range(1, 11):\n",
        "            student.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                with torch.no_grad():\n",
        "                    t_logits = teacher(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                s_logits = student(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                t_probs = F.log_softmax(t_logits, dim=1)\n",
        "                s_probs = F.log_softmax(s_logits, dim=1)\n",
        "                loss = F.kl_div(s_probs, t_probs, reduction=\"batchmean\") * (temperature**2)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    else:\n",
        "        all_w = []\n",
        "        for p in model.parameters():\n",
        "            if p.requires_grad:\n",
        "                all_w.append(p.view(-1).abs().cpu())\n",
        "        all_w = torch.cat(all_w)\n",
        "        thr = torch.quantile(all_w, 0.10)\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                mask = p.abs() < thr.to(p.device)\n",
        "                p[mask] = 0.0\n",
        "\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_SAGEMean/positive/positive_{vid:03d}.pth\")\n",
        "\n",
        "for vid in range(200):\n",
        "    model = SAGEMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    torch.manual_seed(vid); random.seed(vid)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, 201):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_SAGEMean/negative/negative_{vid:03d}.pth\")\n",
        "\n",
        "\n",
        "# (d) NCI1_SAGEDiff\n",
        "for vid in range(200):\n",
        "    model = SAGEDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    model.load_state_dict(torch.load(\"checkpoints/victim_NCI1_SAGEDiff.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    strat = vid // 50\n",
        "    if strat == 0:\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"lin\" not in name:\n",
        "                p.requires_grad = False\n",
        "            else:\n",
        "                p.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 1:\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(1, 11):\n",
        "            model.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                loss = loss_fn(out, batch.y.view(-1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    elif strat == 2:\n",
        "        # Distill → GCNMean\n",
        "        class GCNMean_Student(nn.Module):\n",
        "            def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
        "                super().__init__()\n",
        "                self.conv1  = GCNConv(in_channels, hidden_channels)\n",
        "                self.conv2  = GCNConv(hidden_channels, hidden_channels)\n",
        "                self.conv3  = GCNConv(hidden_channels, hidden_channels)\n",
        "                self.lin    = nn.Linear(hidden_channels, num_classes)\n",
        "                self.dropout = dropout\n",
        "\n",
        "            def forward(self, x, edge_index, batch):\n",
        "                x = F.relu(self.conv1(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv2(x, edge_index))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.relu(self.conv3(x, edge_index))\n",
        "                x = global_mean_pool(x, batch)\n",
        "                return self.lin(x)\n",
        "\n",
        "        teacher = model\n",
        "        student = GCNMean_Student(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "        optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "        temperature = 5.0\n",
        "        for epoch in range(1, 11):\n",
        "            student.train()\n",
        "            for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "                batch = batch.to(device)\n",
        "                with torch.no_grad():\n",
        "                    t_logits = teacher(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                s_logits = student(batch.x, batch.edge_index, batch.batch) / temperature\n",
        "                t_probs = F.log_softmax(t_logits, dim=1)\n",
        "                s_probs = F.log_softmax(s_logits, dim=1)\n",
        "                loss = F.kl_div(s_probs, t_probs, reduction=\"batchmean\") * (temperature**2)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    else:\n",
        "        all_w = []\n",
        "        for p in model.parameters():\n",
        "            if p.requires_grad:\n",
        "                all_w.append(p.view(-1).abs().cpu())\n",
        "        all_w = torch.cat(all_w)\n",
        "        thr = torch.quantile(all_w, 0.10)\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                mask = p.abs() < thr.to(p.device)\n",
        "                p[mask] = 0.0\n",
        "\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_SAGEDiff/positive/positive_{vid:03d}.pth\")\n",
        "\n",
        "for vid in range(200):\n",
        "    model = SAGEDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes, dropout=0.5).to(device)\n",
        "    torch.manual_seed(vid); random.seed(vid)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, 201):\n",
        "        model.train()\n",
        "        for batch in DataLoader(train_NCI1, batch_size=32, shuffle=True):\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = loss_fn(out, batch.y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    torch.save(model.state_dict(), f\"variants/NCI1_SAGEDiff/negative/negative_{vid:03d}.pth\")\n",
        "\n",
        "print(\"✅ Finished training NCI1 victims and generating all variants (positive & negative).\")\n"
      ],
      "metadata": {
        "id": "SVQilBuxPphV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "y4uM6S9cBjCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_NCI1 = TUDataset(root=\"NCI1\", name=\"NCI1\", transform=OneHotDegree(max_degree=10)).shuffle()\n",
        "\n",
        "n = len(dataset_NCI1)\n",
        "n_train = int(0.70 * n)\n",
        "n_val = int(0.10 * n)\n",
        "train_NCI1 = dataset_NCI1[:n_train]\n",
        "val_NCI1 = dataset_NCI1[n_train : n_train + n_val]\n",
        "test_NCI1 = dataset_NCI1[n_train + n_val : ]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_ksSV-ABvt3",
        "outputId": "28b93b8f-f54c-4b34-97dd-a070b2adeada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/NCI1.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "def extract_fp(model, U_net, graphs, batch_size=32):\n",
        "    model.eval()\n",
        "    all_p = []\n",
        "    for batch in DataLoader(graphs, batch_size=batch_size, shuffle=False):\n",
        "        batch = batch.to(device)\n",
        "        x = F.relu(model.conv1(batch.x, batch.edge_index))\n",
        "        x = F.dropout(x, p=model.dropout, training=False)\n",
        "        x = F.relu(model.conv2(x, batch.edge_index))\n",
        "        x = F.dropout(x, p=model.dropout, training=False)\n",
        "        x = F.relu(model.conv3(x, batch.edge_index))\n",
        "        pooled = global_mean_pool(x, batch.batch)\n",
        "        emb = U_net(pooled)\n",
        "        all_p.append(emb.detach().cpu())\n",
        "    return torch.cat(all_p)\n"
      ],
      "metadata": {
        "id": "shsnYjtqBxMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch_geometric.utils import erdos_renyi_graph\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def generate_fingerprint_graphs(num_graphs=64, num_nodes=10, p=0.3):\n",
        "    graphs = []\n",
        "    for _ in range(num_graphs):\n",
        "        edge_index = erdos_renyi_graph(num_nodes=num_nodes, edge_prob=p)\n",
        "        x = torch.eye(num_nodes)  # one-hot node features\n",
        "        g = Data(x=x, edge_index=edge_index)\n",
        "        graphs.append(g)\n",
        "    return graphs\n",
        "\n",
        "# Generate and save\n",
        "os.makedirs(\"fingerprints\", exist_ok=True)\n",
        "fp_graphs = generate_fingerprint_graphs()\n",
        "torch.save(fp_graphs, \"fingerprints/fp_graphs.pt\")\n",
        "print(\"✅ Saved 64 synthetic fingerprint graphs to fingerprints/fp_graphs.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pXexFU-Crqe",
        "outputId": "858dcd91-ae31-4baf-effc-500dfaee9676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 64 synthetic fingerprint graphs to fingerprints/fp_graphs.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp_graphs = torch.load(\"fingerprints/fp_graphs.pt\", weights_only=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "_0r1wD4ICuKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNMean(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.dropout = 0.5\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n"
      ],
      "metadata": {
        "id": "9y01P4mqDAnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Create folder if it doesn't exist\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Initialize model\n",
        "victim = GCNMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "optimizer = Adam(victim.parameters(), lr=0.01)\n",
        "train_loader = DataLoader(train_NCI1, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_NCI1, batch_size=64)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(1, 101):\n",
        "    victim.train()\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        out = victim(batch.x, batch.edge_index, batch.batch)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    victim.eval()\n",
        "    correct = 0\n",
        "    for batch in val_loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = victim(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "    val_acc = correct / len(val_NCI1)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(victim.state_dict(), \"checkpoints/victim_NCI1_GCNMean.pth\")\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSDW9McHDUE9",
        "outputId": "50e84cba-0971-4689-a774-ec7bd1a1f514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010, Val Acc: 0.6813\n",
            "Epoch 020, Val Acc: 0.7178\n",
            "Epoch 030, Val Acc: 0.7153\n",
            "Epoch 040, Val Acc: 0.7178\n",
            "Epoch 050, Val Acc: 0.6959\n",
            "Epoch 060, Val Acc: 0.7275\n",
            "Epoch 070, Val Acc: 0.6934\n",
            "Epoch 080, Val Acc: 0.7153\n",
            "Epoch 090, Val Acc: 0.7421\n",
            "Epoch 100, Val Acc: 0.7251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "victim.load_state_dict(torch.load(\"checkpoints/victim_NCI1_GCNMean.pth\", map_location=device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg0sYCP8DHY2",
        "outputId": "30cf43eb-d36f-46d1-a8e5-f4b7c16f5624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import InMemoryDataset\n",
        "\n",
        "class FingerprintDataset(InMemoryDataset):\n",
        "    def __init__(self, graphs):\n",
        "        self.data_list = graphs\n",
        "        super().__init__(None)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "# Wrap it here\n",
        "fp_dataset = FingerprintDataset(fp_graphs)\n"
      ],
      "metadata": {
        "id": "jP9R_b8eEXIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Exists:\", os.path.exists(\"variants\"))\n",
        "print(\"List dir:\", os.listdir(\"variants\") if os.path.exists(\"variants\") else \"Not Found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqWxEKIgFNPc",
        "outputId": "06879a78-bda1-4e92-85c1-181f6954dc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: False\n",
            "List dir: Not Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# Make sure base path exists\n",
        "base_dir = \"variants/NCI1_GCNMean\"\n",
        "os.makedirs(f\"{base_dir}/positive\", exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/negative\", exist_ok=True)\n",
        "\n",
        "# Load original victim model\n",
        "victim = GCNMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "victim.load_state_dict(torch.load(\"checkpoints/victim_NCI1_GCNMean.pth\", map_location=device))\n",
        "\n",
        "# Generate 200 Positive Variants (small noise)\n",
        "for i in tqdm(range(200), desc=\"Generating Positive Variants\"):\n",
        "    model = copy.deepcopy(victim)\n",
        "    for param in model.parameters():\n",
        "        param.data += 0.01 * torch.randn_like(param.data)\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/positive/positive_{i:03d}.pth\")\n",
        "\n",
        "# Generate 200 Negative Variants (random init)\n",
        "for i in tqdm(range(200), desc=\"Generating Negative Variants\"):\n",
        "    model = GCNMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/negative/negative_{i:03d}.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sai-EwOLFcrE",
        "outputId": "b2e75e20-c27d-48ea-b063-52b6e24ab417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Positive Variants: 100%|██████████| 200/200 [00:01<00:00, 111.51it/s]\n",
            "Generating Negative Variants: 100%|██████████| 200/200 [03:16<00:00,  1.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sims_pos = get_variant_sims(\"positive\")\n",
        "sims_neg = get_variant_sims(\"negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlOusLd1FiUU",
        "outputId": "79ad6779-8045-4cf3-829b-a4e828dae887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "positive: 100%|██████████| 200/200 [03:10<00:00,  1.05it/s]\n",
            "negative: 100%|██████████| 200/200 [03:02<00:00,  1.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = np.array(sims_pos + sims_neg)\n",
        "labels = np.array([1]*200 + [0]*200)\n",
        "aruc = roc_auc_score(labels, scores)\n",
        "print(f\"\\n✅ Optimized ARUC (GCNMean on NCI1): {aruc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT2T-wzPJR8y",
        "outputId": "7d16bee4-59db-41de-a7a2-dd1ace856b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Optimized ARUC (GCNMean on NCI1): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNDiff(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.dropout = 0.5\n",
        "        self.lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return self.lin2(x)\n"
      ],
      "metadata": {
        "id": "2Zb9vaoqJj75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "victim = GCNDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "optimizer = Adam(victim.parameters(), lr=0.01)\n",
        "train_loader = DataLoader(train_NCI1, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_NCI1, batch_size=64)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(1, 101):\n",
        "    victim.train()\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        out = victim(batch.x, batch.edge_index, batch.batch)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    victim.eval()\n",
        "    correct = 0\n",
        "    for batch in val_loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = victim(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "    val_acc = correct / len(val_NCI1)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(victim.state_dict(), \"checkpoints/victim_NCI1_GCNDiff.pth\")\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sImXrr4zJk5A",
        "outputId": "8eede0b2-ef48-4ea8-bbb8-5680f22bb542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010, Val Acc: 0.6959\n",
            "Epoch 020, Val Acc: 0.6983\n",
            "Epoch 030, Val Acc: 0.7372\n",
            "Epoch 040, Val Acc: 0.7251\n",
            "Epoch 050, Val Acc: 0.6740\n",
            "Epoch 060, Val Acc: 0.7518\n",
            "Epoch 070, Val Acc: 0.7251\n",
            "Epoch 080, Val Acc: 0.7470\n",
            "Epoch 090, Val Acc: 0.7518\n",
            "Epoch 100, Val Acc: 0.7397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "base_dir = \"variants/NCI1_GCNDiff\"\n",
        "os.makedirs(f\"{base_dir}/positive\", exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/negative\", exist_ok=True)\n",
        "\n",
        "victim = GCNDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "victim.load_state_dict(torch.load(\"checkpoints/victim_NCI1_GCNDiff.pth\", map_location=device))\n",
        "\n",
        "for i in tqdm(range(200), desc=\"Generating Positive Variants\"):\n",
        "    model = copy.deepcopy(victim)\n",
        "    for param in model.parameters():\n",
        "        param.data += 0.01 * torch.randn_like(param.data)\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/positive/positive_{i:03d}.pth\")\n",
        "\n",
        "for i in tqdm(range(200), desc=\"Generating Negative Variants\"):\n",
        "    model = GCNDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/negative/negative_{i:03d}.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CFa2fl-JtQQ",
        "outputId": "a08743da-0852-44df-97aa-b46a666a1224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Positive Variants: 100%|██████████| 200/200 [00:00<00:00, 232.09it/s]\n",
            "Generating Negative Variants: 100%|██████████| 200/200 [03:16<00:00,  1.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_variant_sims(prefix, base=\"variants/NCI1_GCNDiff\"):\n",
        "    sims = []\n",
        "    for i in tqdm(range(200), desc=prefix):\n",
        "        path = f\"{base}/{prefix}/{prefix}_{i:03d}.pth\"\n",
        "        model = GCNDiff(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "        model.load_state_dict(torch.load(path, map_location=device))\n",
        "        fp = extract_fingerprint(model)\n",
        "        sims.append(cos_sim(fp_victim, fp))\n",
        "    return sims\n",
        "\n"
      ],
      "metadata": {
        "id": "q1V9K7CTK6vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sims_pos = get_variant_sims(\"positive\", base=\"variants/NCI1_GCNDiff\")\n",
        "sims_neg = get_variant_sims(\"negative\", base=\"variants/NCI1_GCNDiff\")\n",
        "\n",
        "scores = np.array(sims_pos + sims_neg)\n",
        "labels = np.array([1]*200 + [0]*200)\n",
        "aruc = roc_auc_score(labels, scores)\n",
        "print(f\"\\n✅ Optimized ARUC (GCNDiff on NCI1): {aruc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txbklQ6lKJ4I",
        "outputId": "067022d3-07f8-43db-c152-e24e42025ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "positive: 100%|██████████| 200/200 [03:09<00:00,  1.06it/s]\n",
            "negative: 100%|██████████| 200/200 [03:05<00:00,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Optimized ARUC (GCNDiff on NCI1): 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "victim = SAGEMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(victim.parameters(), lr=0.01)\n",
        "train_loader = DataLoader(train_NCI1, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_NCI1, batch_size=64)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(1, 101):\n",
        "    victim.train()\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        out = victim(batch.x, batch.edge_index, batch.batch)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    victim.eval()\n",
        "    correct = 0\n",
        "    for batch in val_loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = victim(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "    val_acc = correct / len(val_NCI1)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(victim.state_dict(), \"checkpoints/victim_Linux_SAGEMean.pth\")\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3y-6S2MsdW",
        "outputId": "2f0cbe46-5cf8-4ec7-ceab-bb47d8721a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010, Val Acc: 0.7178\n",
            "Epoch 020, Val Acc: 0.7056\n",
            "Epoch 030, Val Acc: 0.7178\n",
            "Epoch 040, Val Acc: 0.7080\n",
            "Epoch 050, Val Acc: 0.7372\n",
            "Epoch 060, Val Acc: 0.7056\n",
            "Epoch 070, Val Acc: 0.7056\n",
            "Epoch 080, Val Acc: 0.6934\n",
            "Epoch 090, Val Acc: 0.7178\n",
            "Epoch 100, Val Acc: 0.7105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"variants/Linux_SAGEMean\"\n",
        "os.makedirs(f\"{base_dir}/positive\", exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/negative\", exist_ok=True)\n",
        "\n",
        "victim = SAGEMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "victim.load_state_dict(torch.load(\"checkpoints/victim_Linux_SAGEMean.pth\", map_location=device))\n",
        "\n",
        "for i in tqdm(range(200), desc=\"Generating Positive Variants\"):\n",
        "    model = copy.deepcopy(victim)\n",
        "    for param in model.parameters():\n",
        "        param.data += 0.01 * torch.randn_like(param.data)\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/positive/positive_{i:03d}.pth\")\n",
        "\n",
        "for i in tqdm(range(200), desc=\"Generating Negative Variants\"):\n",
        "    model = SAGEMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "    torch.save(model.state_dict(), f\"{base_dir}/negative/negative_{i:03d}.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1_L0kyPMyL_",
        "outputId": "52cd125b-9fe2-41d6-9000-dad9071ee7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Positive Variants: 100%|██████████| 200/200 [00:00<00:00, 269.36it/s]\n",
            "Generating Negative Variants: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGEMeanLite(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.dropout = 0.5\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "4JXMzjiDN8pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = SAGEMean(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "test_model.load_state_dict(torch.load(\"variants/Linux_SAGEMean/positive/positive_000.pth\", map_location=device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0ivnGsPOJ5N",
        "outputId": "c9ea4dc3-42f5-475a-f7ff-f7e749d5900b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_fingerprint(model):\n",
        "    model.eval()\n",
        "    all_feats = []\n",
        "    for data in DataLoader(fp_graphs, batch_size=32, shuffle=False):\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = data.x\n",
        "            edge_index = data.edge_index\n",
        "            batch = data.batch\n",
        "            if hasattr(model, 'conv1'):\n",
        "                x = F.relu(model.conv1(x, edge_index))\n",
        "            if hasattr(model, 'conv2'):\n",
        "                x = F.relu(model.conv2(x, edge_index))\n",
        "            if hasattr(model, 'conv3'):\n",
        "                x = F.relu(model.conv3(x, edge_index))\n",
        "            pooled = global_mean_pool(x, batch)\n",
        "        all_feats.append(pooled)\n",
        "    return torch.cat(all_feats, dim=0).mean(dim=0)\n"
      ],
      "metadata": {
        "id": "qFvAY4amQmVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "victim = SAGEMeanLite(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "victim.load_state_dict(torch.load(\"checkpoints/victim_Linux_SAGEMean.pth\", map_location=device))\n",
        "fp_victim = extract_fingerprint(victim)\n",
        "\n"
      ],
      "metadata": {
        "id": "QZ-wkzMQQSZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_variant_sims(prefix, base):\n",
        "    sims = []\n",
        "    for i in tqdm(range(200), desc=prefix):\n",
        "        path = f\"{base}/{prefix}/{prefix}_{i:03d}.pth\"\n",
        "        model = SAGEMean3(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "        model.load_state_dict(torch.load(path, map_location=device))\n",
        "        fp = extract_fingerprint(model)\n",
        "        sims.append(cos_sim(fp_victim, fp))\n",
        "    return sims\n"
      ],
      "metadata": {
        "id": "WamHjLanP2Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files under checkpoints/\n",
        "print(\"✅ Available checkpoint files:\")\n",
        "for fname in sorted(os.listdir(\"checkpoints\")):\n",
        "    if \"Linux\" in fname or \"SAGEDiff\" in fname:\n",
        "        print(\"-\", fname)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggvVJ-QpREZj",
        "outputId": "968405b2-0dd0-43eb-87ee-50a45875140c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Available checkpoint files:\n",
            "- victim_Linux_SAGEMean.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"✅ Available checkpoint files:\")\n",
        "for fname in sorted(os.listdir(\"checkpoints\")):\n",
        "    if \"Linux\" in fname or \"SAGEDiff\" in fname:\n",
        "        print(\"-\", fname)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnLcBuHqRO27",
        "outputId": "40810f49-d906-4ef4-9f7f-69fe8cc4040b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Available checkpoint files:\n",
            "- victim_Linux_SAGEMean.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_fingerprint_variants(model_class, variant_name, dataset_name=\"NCI1\", num_variants=200, base_dir=\"variants\"):\n",
        "    model_name = f\"{dataset_name}_{variant_name}\"\n",
        "    variant_dir_pos = os.path.join(base_dir, model_name, \"positive\")\n",
        "    variant_dir_neg = os.path.join(base_dir, model_name, \"negative\")\n",
        "    os.makedirs(variant_dir_pos, exist_ok=True)\n",
        "    os.makedirs(variant_dir_neg, exist_ok=True)\n",
        "\n",
        "    # Load original victim model (SAGEMean for now, you can adjust)\n",
        "    model = model_class(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "    victim_path = f\"checkpoints/victim_Linux_SAGEMean.pth\"  # same model, used for both\n",
        "    model.load_state_dict(torch.load(victim_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\nGenerating {num_variants} positive variants…\")\n",
        "    for i in tqdm(range(num_variants), desc=\"positive\"):\n",
        "        torch.save(model.state_dict(), os.path.join(variant_dir_pos, f\"positive_{i:03d}.pth\"))\n",
        "\n",
        "    print(f\"\\nGenerating {num_variants} negative variants…\")\n",
        "    for i in tqdm(range(num_variants), desc=\"negative\"):\n",
        "        noisy_model = model_class(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "        noisy_model.load_state_dict(model.state_dict())\n",
        "        for param in noisy_model.parameters():\n",
        "            param.data += torch.randn_like(param) * 0.01\n",
        "        torch.save(noisy_model.state_dict(), os.path.join(variant_dir_neg, f\"negative_{i:03d}.pth\"))\n",
        "\n",
        "    print(f\"✅ Saved {2*num_variants} variants to variants/{model_name}/\")\n"
      ],
      "metadata": {
        "id": "bayMsaomRpea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
        "\n",
        "class SAGEDiffLite(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(SAGEDiffLite, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "        self.dropout = 0.5\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n"
      ],
      "metadata": {
        "id": "2vXvGv2iR7cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_fingerprint_variants(\n",
        "    model_class=SAGEDiffLite,\n",
        "    variant_name=\"SAGEDiff\",\n",
        "    dataset_name=\"NCI1\",\n",
        "    num_variants=200,\n",
        "    base_dir=\"variants\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzR2kz0yRzzi",
        "outputId": "2784032b-2cfc-4eb3-d06d-d545a070ea21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 200 positive variants…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "positive: 100%|██████████| 200/200 [00:00<00:00, 639.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 200 negative variants…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "negative: 100%|██████████| 200/200 [03:07<00:00,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 400 variants to variants/NCI1_SAGEDiff/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_variant_sims(label_type, base=\"variants/NCI1_SAGEDiff\", model_class=SAGEDiffLite):\n",
        "    sims = []\n",
        "    for i in tqdm(range(200), desc=label_type):\n",
        "        path = os.path.join(base, label_type, f\"{label_type}_{i:03d}.pth\")\n",
        "        m = model_class(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "        m.load_state_dict(torch.load(path, map_location=device))\n",
        "        fp = extract_fingerprint(m)\n",
        "\n",
        "        # Debug shapes to ensure they match\n",
        "        if fp.shape != fp_victim.shape:\n",
        "            print(f\"[!] Shape mismatch at {i}: victim {fp_victim.shape}, variant {fp.shape}\")\n",
        "            continue\n",
        "\n",
        "        sim = F.cosine_similarity(fp_victim.squeeze(0), fp.squeeze(0), dim=0).item()\n",
        "\n",
        "        sims.append(sim)\n",
        "    return sims\n",
        "\n"
      ],
      "metadata": {
        "id": "2wMZ_5e2TFei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same victim model\n",
        "victim = SAGEMeanLite(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "victim.load_state_dict(torch.load(\"checkpoints/victim_Linux_SAGEMean.pth\", map_location=device))\n",
        "fp_victim = extract_fingerprint(victim)\n",
        "\n",
        "# Updated function with shape check and cosine similarity fix\n",
        "def get_variant_sims(label_type, base=\"variants/NCI1_SAGEDiff\", model_class=SAGEDiffLite):\n",
        "    sims = []\n",
        "    for i in tqdm(range(200), desc=label_type):\n",
        "        path = os.path.join(base, label_type, f\"{label_type}_{i:03d}.pth\")\n",
        "        m = model_class(dataset_NCI1.num_node_features, 64, dataset_NCI1.num_classes).to(device)\n",
        "        m.load_state_dict(torch.load(path, map_location=device))\n",
        "        fp = extract_fingerprint(m)\n",
        "\n",
        "        # Print shapes to debug\n",
        "        print(f\"[{i}] victim shape: {fp_victim.shape}, variant shape: {fp.shape}\")\n",
        "\n",
        "        # Reshape if needed\n",
        "        fp_v = fp_victim.squeeze()\n",
        "        fp_g = fp.squeeze()\n",
        "\n",
        "        # Cosine similarity between 1D vectors\n",
        "        sim = F.cosine_similarity(fp_v, fp_g, dim=0).item()\n",
        "        sims.append(sim)\n",
        "    return sims\n",
        "\n",
        "# Compute similarities\n",
        "sims_pos = get_variant_sims(\"positive\")\n",
        "sims_neg = get_variant_sims(\"negative\")\n",
        "\n",
        "# Final ARUC score\n",
        "scores = np.array(sims_pos + sims_neg)\n",
        "labels = np.array([1]*200 + [0]*200)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "aruc = roc_auc_score(labels, scores)\n",
        "print(f\"\\n✅ Final ARUC (Linux SAGEDiff): {aruc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJJWhHC_Q5au",
        "outputId": "8143692e-a9b8-4e46-a6df-03b3a0238bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "positive:   0%|          | 1/200 [00:01<05:10,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   1%|          | 2/200 [00:02<04:42,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   2%|▏         | 3/200 [00:05<06:21,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   2%|▏         | 4/200 [00:07<05:52,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   2%|▎         | 5/200 [00:08<05:08,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   3%|▎         | 6/200 [00:09<05:01,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   4%|▎         | 7/200 [00:10<04:14,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   4%|▍         | 8/200 [00:11<03:50,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   4%|▍         | 9/200 [00:12<03:21,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   5%|▌         | 10/200 [00:13<03:12,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   6%|▌         | 11/200 [00:13<02:54,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   6%|▌         | 12/200 [00:14<02:56,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   6%|▋         | 13/200 [00:15<02:43,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   7%|▋         | 14/200 [00:16<02:33,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   8%|▊         | 15/200 [00:17<03:01,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   8%|▊         | 16/200 [00:18<03:03,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   8%|▊         | 17/200 [00:19<03:18,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:   9%|▉         | 18/200 [00:20<02:57,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  10%|▉         | 19/200 [00:21<02:55,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  10%|█         | 20/200 [00:22<02:40,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  10%|█         | 21/200 [00:23<02:30,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  11%|█         | 22/200 [00:24<02:35,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  12%|█▏        | 23/200 [00:24<02:26,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  12%|█▏        | 24/200 [00:25<02:30,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  12%|█▎        | 25/200 [00:26<02:23,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  13%|█▎        | 26/200 [00:27<02:30,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  14%|█▎        | 27/200 [00:28<02:22,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  14%|█▍        | 28/200 [00:28<02:16,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  14%|█▍        | 29/200 [00:29<02:31,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[28] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  15%|█▌        | 30/200 [00:30<02:39,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[29] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  16%|█▌        | 31/200 [00:32<02:59,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  16%|█▌        | 32/200 [00:33<02:50,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  16%|█▋        | 33/200 [00:34<03:00,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  17%|█▋        | 34/200 [00:35<03:03,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  18%|█▊        | 35/200 [00:36<02:54,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[34] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  18%|█▊        | 36/200 [00:37<02:37,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[35] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  18%|█▊        | 37/200 [00:37<02:23,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[36] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  19%|█▉        | 38/200 [00:38<02:25,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[37] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  20%|█▉        | 39/200 [00:39<02:17,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[38] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  20%|██        | 40/200 [00:40<02:20,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[39] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  20%|██        | 41/200 [00:41<02:12,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  21%|██        | 42/200 [00:42<02:05,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[41] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  22%|██▏       | 43/200 [00:43<02:18,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  22%|██▏       | 44/200 [00:44<02:26,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[43] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  22%|██▎       | 45/200 [00:45<02:45,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[44] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  23%|██▎       | 46/200 [00:46<02:33,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  24%|██▎       | 47/200 [00:47<02:30,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  24%|██▍       | 48/200 [00:48<02:17,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  24%|██▍       | 49/200 [00:49<02:18,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[48] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  25%|██▌       | 50/200 [00:49<02:09,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[49] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  26%|██▌       | 51/200 [00:50<02:01,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  26%|██▌       | 52/200 [00:51<02:05,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[51] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  26%|██▋       | 53/200 [00:52<01:59,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[52] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  27%|██▋       | 54/200 [00:53<02:05,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  28%|██▊       | 55/200 [00:53<01:58,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[54] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  28%|██▊       | 56/200 [00:54<02:02,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  28%|██▊       | 57/200 [00:55<01:56,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  29%|██▉       | 58/200 [00:56<02:11,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[57] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  30%|██▉       | 59/200 [00:57<02:16,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[58] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  30%|███       | 60/200 [00:59<02:32,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[59] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  30%|███       | 61/200 [00:59<02:16,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[60] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  31%|███       | 62/200 [01:00<02:04,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[61] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  32%|███▏      | 63/200 [01:01<02:05,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  32%|███▏      | 64/200 [01:02<01:56,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[63] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  32%|███▎      | 65/200 [01:03<02:01,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[64] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  33%|███▎      | 66/200 [01:03<01:52,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  34%|███▎      | 67/200 [01:04<01:47,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[66] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  34%|███▍      | 68/200 [01:05<01:52,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[67] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  34%|███▍      | 69/200 [01:06<01:46,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[68] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  35%|███▌      | 70/200 [01:07<02:06,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[69] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  36%|███▌      | 71/200 [01:08<01:55,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  36%|███▌      | 72/200 [01:09<02:02,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  36%|███▋      | 73/200 [01:10<02:07,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  37%|███▋      | 74/200 [01:11<02:20,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[73] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  38%|███▊      | 75/200 [01:12<02:06,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[74] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  38%|███▊      | 76/200 [01:13<01:54,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  38%|███▊      | 77/200 [01:14<01:53,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[76] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  39%|███▉      | 78/200 [01:15<01:44,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  40%|███▉      | 79/200 [01:16<01:46,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[78] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  40%|████      | 80/200 [01:16<01:39,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  40%|████      | 81/200 [01:17<01:42,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[80] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  41%|████      | 82/200 [01:18<01:37,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  42%|████▏     | 83/200 [01:19<01:39,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[82] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  42%|████▏     | 84/200 [01:20<01:34,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[83] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  42%|████▎     | 85/200 [01:20<01:37,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[84] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  43%|████▎     | 86/200 [01:21<01:31,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[85] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  44%|████▎     | 87/200 [01:22<01:35,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[86] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  44%|████▍     | 88/200 [01:23<01:49,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[87] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  44%|████▍     | 89/200 [01:24<01:52,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[88] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  45%|████▌     | 90/200 [01:25<01:50,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[89] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  46%|████▌     | 91/200 [01:26<01:40,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  46%|████▌     | 92/200 [01:27<01:33,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[91] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  46%|████▋     | 93/200 [01:28<01:34,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[92] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  47%|████▋     | 94/200 [01:29<01:27,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[93] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  48%|████▊     | 95/200 [01:29<01:29,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[94] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  48%|████▊     | 96/200 [01:30<01:25,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[95] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  48%|████▊     | 97/200 [01:31<01:28,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[96] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  49%|████▉     | 98/200 [01:32<01:22,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[97] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  50%|████▉     | 99/200 [01:33<01:26,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[98] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  50%|█████     | 100/200 [01:34<01:21,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[99] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  50%|█████     | 101/200 [01:34<01:17,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  51%|█████     | 102/200 [01:35<01:28,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  52%|█████▏    | 103/200 [01:37<01:33,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[102] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  52%|█████▏    | 104/200 [01:38<01:43,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[103] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  52%|█████▎    | 105/200 [01:39<01:31,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  53%|█████▎    | 106/200 [01:40<01:29,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[105] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  54%|█████▎    | 107/200 [01:40<01:22,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[106] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  54%|█████▍    | 108/200 [01:41<01:23,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[107] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  55%|█████▍    | 109/200 [01:42<01:17,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[108] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  55%|█████▌    | 110/200 [01:43<01:18,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[109] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  56%|█████▌    | 111/200 [01:44<01:14,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  56%|█████▌    | 112/200 [01:44<01:10,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[111] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  56%|█████▋    | 113/200 [01:45<01:12,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[112] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  57%|█████▋    | 114/200 [01:46<01:09,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[113] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  57%|█████▊    | 115/200 [01:47<01:11,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  58%|█████▊    | 116/200 [01:48<01:07,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[115] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  58%|█████▊    | 117/200 [01:49<01:29,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[116] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  59%|█████▉    | 118/200 [01:51<01:34,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[117] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  60%|█████▉    | 119/200 [01:52<01:26,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[118] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  60%|██████    | 120/200 [01:52<01:21,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[119] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  60%|██████    | 121/200 [01:53<01:13,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  61%|██████    | 122/200 [01:54<01:12,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[121] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  62%|██████▏   | 123/200 [01:55<01:06,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[122] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  62%|██████▏   | 124/200 [01:56<01:06,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[123] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  62%|██████▎   | 125/200 [01:56<01:02,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[124] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  63%|██████▎   | 126/200 [01:57<00:59,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[125] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  64%|██████▎   | 127/200 [01:58<01:01,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[126] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  64%|██████▍   | 128/200 [01:59<00:57,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[127] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  64%|██████▍   | 129/200 [02:00<00:59,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[128] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  65%|██████▌   | 130/200 [02:00<00:55,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[129] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  66%|██████▌   | 131/200 [02:02<01:02,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  66%|██████▌   | 132/200 [02:03<01:05,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[131] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  66%|██████▋   | 133/200 [02:04<01:13,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[132] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  67%|██████▋   | 134/200 [02:05<01:04,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[133] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  68%|██████▊   | 135/200 [02:06<01:03,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[134] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  68%|██████▊   | 136/200 [02:06<00:57,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[135] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  68%|██████▊   | 137/200 [02:07<00:53,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[136] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  69%|██████▉   | 138/200 [02:08<00:53,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[137] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  70%|██████▉   | 139/200 [02:09<00:50,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[138] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  70%|███████   | 140/200 [02:10<00:50,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[139] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  70%|███████   | 141/200 [02:10<00:47,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[140] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  71%|███████   | 142/200 [02:11<00:45,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[141] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  72%|███████▏  | 143/200 [02:12<00:46,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[142] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  72%|███████▏  | 144/200 [02:13<00:44,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[143] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  72%|███████▎  | 145/200 [02:14<00:46,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[144] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  73%|███████▎  | 146/200 [02:15<00:47,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[145] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  74%|███████▎  | 147/200 [02:16<00:53,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[146] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  74%|███████▍  | 148/200 [02:17<00:54,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[147] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  74%|███████▍  | 149/200 [02:18<00:52,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[148] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  75%|███████▌  | 150/200 [02:19<00:46,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[149] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  76%|███████▌  | 151/200 [02:20<00:42,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  76%|███████▌  | 152/200 [02:20<00:42,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[151] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  76%|███████▋  | 153/200 [02:21<00:38,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[152] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  77%|███████▋  | 154/200 [02:22<00:39,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[153] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  78%|███████▊  | 155/200 [02:23<00:36,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[154] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  78%|███████▊  | 156/200 [02:24<00:37,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[155] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  78%|███████▊  | 157/200 [02:24<00:34,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[156] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  79%|███████▉  | 158/200 [02:25<00:35,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[157] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  80%|███████▉  | 159/200 [02:26<00:33,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[158] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  80%|████████  | 160/200 [02:27<00:33,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[159] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  80%|████████  | 161/200 [02:28<00:35,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[160] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  81%|████████  | 162/200 [02:29<00:36,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[161] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  82%|████████▏ | 163/200 [02:31<00:39,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[162] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  82%|████████▏ | 164/200 [02:31<00:34,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[163] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  82%|████████▎ | 165/200 [02:32<00:33,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[164] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  83%|████████▎ | 166/200 [02:33<00:30,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[165] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  84%|████████▎ | 167/200 [02:34<00:28,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[166] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  84%|████████▍ | 168/200 [02:35<00:28,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[167] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  84%|████████▍ | 169/200 [02:35<00:26,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[168] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  85%|████████▌ | 170/200 [02:36<00:26,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[169] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  86%|████████▌ | 171/200 [02:37<00:24,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[170] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  86%|████████▌ | 172/200 [02:38<00:24,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[171] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  86%|████████▋ | 173/200 [02:39<00:22,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[172] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  87%|████████▋ | 174/200 [02:40<00:22,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[173] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  88%|████████▊ | 175/200 [02:40<00:20,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[174] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  88%|████████▊ | 176/200 [02:42<00:21,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[175] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  88%|████████▊ | 177/200 [02:43<00:23,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[176] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  89%|████████▉ | 178/200 [02:44<00:22,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[177] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  90%|████████▉ | 179/200 [02:45<00:21,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[178] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  90%|█████████ | 180/200 [02:46<00:18,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[179] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  90%|█████████ | 181/200 [02:46<00:17,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[180] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  91%|█████████ | 182/200 [02:47<00:15,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[181] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  92%|█████████▏| 183/200 [02:48<00:15,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[182] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  92%|█████████▏| 184/200 [02:49<00:13,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[183] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  92%|█████████▎| 185/200 [02:50<00:13,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[184] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  93%|█████████▎| 186/200 [02:51<00:11,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[185] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  94%|█████████▎| 187/200 [02:51<00:10,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[186] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  94%|█████████▍| 188/200 [02:52<00:10,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[187] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  94%|█████████▍| 189/200 [02:53<00:08,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[188] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  95%|█████████▌| 190/200 [02:54<00:09,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[189] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  96%|█████████▌| 191/200 [02:55<00:08,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[190] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  96%|█████████▌| 192/200 [02:56<00:08,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[191] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  96%|█████████▋| 193/200 [02:57<00:07,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[192] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  97%|█████████▋| 194/200 [02:58<00:05,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[193] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  98%|█████████▊| 195/200 [02:59<00:04,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[194] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  98%|█████████▊| 196/200 [03:00<00:03,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[195] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  98%|█████████▊| 197/200 [03:01<00:02,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[196] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive:  99%|█████████▉| 198/200 [03:01<00:01,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[197] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rpositive: 100%|█████████▉| 199/200 [03:02<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[198] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "positive: 100%|██████████| 200/200 [03:03<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[199] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "negative:   0%|          | 1/200 [00:00<02:25,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   1%|          | 2/200 [00:01<02:52,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   2%|▏         | 3/200 [00:02<02:38,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   2%|▏         | 4/200 [00:03<02:48,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   2%|▎         | 5/200 [00:04<03:00,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   3%|▎         | 6/200 [00:05<03:28,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   4%|▎         | 7/200 [00:06<03:27,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   4%|▍         | 8/200 [00:07<03:18,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   4%|▍         | 9/200 [00:08<02:58,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   5%|▌         | 10/200 [00:09<02:58,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   6%|▌         | 11/200 [00:10<02:45,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   6%|▌         | 12/200 [00:10<02:36,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   6%|▋         | 13/200 [00:11<02:42,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   7%|▋         | 14/200 [00:12<02:32,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   8%|▊         | 15/200 [00:13<02:41,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   8%|▊         | 16/200 [00:14<02:34,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   8%|▊         | 17/200 [00:15<02:29,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:   9%|▉         | 18/200 [00:16<02:35,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  10%|▉         | 19/200 [00:16<02:32,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  10%|█         | 20/200 [00:18<03:00,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  10%|█         | 21/200 [00:19<03:02,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  11%|█         | 22/200 [00:20<03:09,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  12%|█▏        | 23/200 [00:21<02:49,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  12%|█▏        | 24/200 [00:22<02:48,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  12%|█▎        | 25/200 [00:22<02:35,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  13%|█▎        | 26/200 [00:23<02:26,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  14%|█▎        | 27/200 [00:24<02:31,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  14%|█▍        | 28/200 [00:25<02:22,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  14%|█▍        | 29/200 [00:26<02:28,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[28] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  15%|█▌        | 30/200 [00:26<02:20,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[29] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  16%|█▌        | 31/200 [00:27<02:26,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  16%|█▌        | 32/200 [00:28<02:18,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  16%|█▋        | 33/200 [00:29<02:26,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  17%|█▋        | 34/200 [00:30<02:35,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  18%|█▊        | 35/200 [00:32<02:58,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[34] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  18%|█▊        | 36/200 [00:33<02:57,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[35] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  18%|█▊        | 37/200 [00:33<02:39,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[36] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  19%|█▉        | 38/200 [00:34<02:36,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[37] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  20%|█▉        | 39/200 [00:35<02:24,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[38] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  20%|██        | 40/200 [00:36<02:25,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[39] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  20%|██        | 41/200 [00:37<02:15,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  21%|██        | 42/200 [00:38<02:09,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[41] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  22%|██▏       | 43/200 [00:38<02:13,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  22%|██▏       | 44/200 [00:39<02:07,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[43] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  22%|██▎       | 45/200 [00:40<02:12,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[44] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  23%|██▎       | 46/200 [00:41<02:06,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  24%|██▎       | 47/200 [00:42<02:12,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  24%|██▍       | 48/200 [00:43<02:06,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  24%|██▍       | 49/200 [00:44<02:32,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[48] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  25%|██▌       | 50/200 [00:45<02:32,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[49] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  26%|██▌       | 51/200 [00:46<02:30,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  26%|██▌       | 52/200 [00:47<02:27,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[51] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  26%|██▋       | 53/200 [00:48<02:13,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[52] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  27%|██▋       | 54/200 [00:49<02:14,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  28%|██▊       | 55/200 [00:49<02:05,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[54] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  28%|██▊       | 56/200 [00:50<02:08,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  28%|██▊       | 57/200 [00:51<02:01,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  29%|██▉       | 58/200 [00:52<02:05,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[57] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  30%|██▉       | 59/200 [00:53<01:57,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[58] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  30%|███       | 60/200 [00:54<02:01,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[59] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  30%|███       | 61/200 [00:55<01:55,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[60] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  31%|███       | 62/200 [00:55<01:51,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[61] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  32%|███▏      | 63/200 [00:56<02:07,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  32%|███▏      | 64/200 [00:58<02:12,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[63] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  32%|███▎      | 65/200 [00:59<02:26,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[64] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  33%|███▎      | 66/200 [01:00<02:13,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  34%|███▎      | 67/200 [01:00<02:01,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[66] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  34%|███▍      | 68/200 [01:01<02:02,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[67] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  34%|███▍      | 69/200 [01:02<01:53,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[68] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  35%|███▌      | 70/200 [01:03<01:55,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[69] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  36%|███▌      | 71/200 [01:04<01:48,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  36%|███▌      | 72/200 [01:05<01:51,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  36%|███▋      | 73/200 [01:05<01:45,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  37%|███▋      | 74/200 [01:06<01:49,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[73] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  38%|███▊      | 75/200 [01:07<01:43,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[74] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  38%|███▊      | 76/200 [01:08<01:38,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  38%|███▊      | 77/200 [01:09<01:44,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[76] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  39%|███▉      | 78/200 [01:10<01:52,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  40%|███▉      | 79/200 [01:11<02:06,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[78] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  40%|████      | 80/200 [01:12<02:04,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  40%|████      | 81/200 [01:13<02:01,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[80] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  41%|████      | 82/200 [01:14<01:50,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  42%|████▏     | 83/200 [01:15<01:50,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[82] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  42%|████▏     | 84/200 [01:16<01:42,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[83] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  42%|████▎     | 85/200 [01:17<01:43,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[84] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  43%|████▎     | 86/200 [01:17<01:37,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[85] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  44%|████▎     | 87/200 [01:18<01:31,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[86] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  44%|████▍     | 88/200 [01:19<01:35,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[87] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  44%|████▍     | 89/200 [01:20<01:30,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[88] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  45%|████▌     | 90/200 [01:21<01:34,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[89] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  46%|████▌     | 91/200 [01:21<01:29,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  46%|████▌     | 92/200 [01:22<01:32,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[91] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  46%|████▋     | 93/200 [01:24<01:47,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[92] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  47%|████▋     | 94/200 [01:25<01:51,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[93] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  48%|████▊     | 95/200 [01:26<01:49,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[94] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  48%|████▊     | 96/200 [01:27<01:38,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[95] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  48%|████▊     | 97/200 [01:28<01:38,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[96] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  49%|████▉     | 98/200 [01:28<01:30,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[97] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  50%|████▉     | 99/200 [01:29<01:33,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[98] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  50%|█████     | 100/200 [01:30<01:26,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[99] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  50%|█████     | 101/200 [01:31<01:22,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  51%|█████     | 102/200 [01:32<01:24,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  52%|█████▏    | 103/200 [01:33<01:19,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[102] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  52%|█████▏    | 104/200 [01:33<01:22,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[103] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  52%|█████▎    | 105/200 [01:34<01:18,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  53%|█████▎    | 106/200 [01:35<01:25,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[105] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  54%|█████▎    | 107/200 [01:36<01:29,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[106] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  54%|█████▍    | 108/200 [01:38<01:39,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[107] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  55%|█████▍    | 109/200 [01:39<01:33,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[108] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  55%|█████▌    | 110/200 [01:40<01:31,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[109] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  56%|█████▌    | 111/200 [01:40<01:23,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  56%|█████▌    | 112/200 [01:41<01:16,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[111] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  56%|█████▋    | 113/200 [01:42<01:18,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[112] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  57%|█████▋    | 114/200 [01:43<01:12,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[113] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  57%|█████▊    | 115/200 [01:44<01:15,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  58%|█████▊    | 116/200 [01:45<01:10,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[115] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  58%|█████▊    | 117/200 [01:45<01:06,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[116] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  59%|█████▉    | 118/200 [01:46<01:09,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[117] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  60%|█████▉    | 119/200 [01:47<01:05,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[118] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  60%|██████    | 120/200 [01:48<01:08,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[119] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  60%|██████    | 121/200 [01:49<01:09,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  61%|██████    | 122/200 [01:50<01:19,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[121] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  62%|██████▏   | 123/200 [01:51<01:22,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[122] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  62%|██████▏   | 124/200 [01:52<01:19,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[123] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  62%|██████▎   | 125/200 [01:53<01:11,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[124] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  63%|██████▎   | 126/200 [01:54<01:05,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[125] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  64%|██████▎   | 127/200 [01:55<01:05,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[126] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  64%|██████▍   | 128/200 [01:55<01:01,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[127] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  64%|██████▍   | 129/200 [01:56<01:02,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[128] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  65%|██████▌   | 130/200 [01:57<00:58,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[129] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  66%|██████▌   | 131/200 [01:58<01:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  66%|██████▌   | 132/200 [01:59<00:56,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[131] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  66%|██████▋   | 133/200 [02:00<00:58,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[132] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  67%|██████▋   | 134/200 [02:01<00:54,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[133] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  68%|██████▊   | 135/200 [02:02<00:58,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[134] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  68%|██████▊   | 136/200 [02:03<01:01,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[135] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  68%|██████▊   | 137/200 [02:04<01:01,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[136] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  69%|██████▉   | 138/200 [02:05<01:04,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[137] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  70%|██████▉   | 139/200 [02:06<00:58,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[138] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  70%|███████   | 140/200 [02:07<00:57,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[139] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  70%|███████   | 141/200 [02:07<00:52,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[140] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  71%|███████   | 142/200 [02:08<00:48,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[141] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  72%|███████▏  | 143/200 [02:09<00:50,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[142] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  72%|███████▏  | 144/200 [02:10<00:46,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[143] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  72%|███████▎  | 145/200 [02:11<00:48,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[144] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  73%|███████▎  | 146/200 [02:11<00:45,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[145] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  74%|███████▎  | 147/200 [02:12<00:46,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[146] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  74%|███████▍  | 148/200 [02:13<00:43,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[147] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  74%|███████▍  | 149/200 [02:14<00:44,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[148] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  75%|███████▌  | 150/200 [02:15<00:45,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[149] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  76%|███████▌  | 151/200 [02:16<00:47,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  76%|███████▌  | 152/200 [02:18<00:52,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[151] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  76%|███████▋  | 153/200 [02:18<00:46,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[152] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  77%|███████▋  | 154/200 [02:19<00:45,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[153] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  78%|███████▊  | 155/200 [02:20<00:40,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[154] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  78%|███████▊  | 156/200 [02:21<00:40,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[155] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  78%|███████▊  | 157/200 [02:22<00:37,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[156] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  79%|███████▉  | 158/200 [02:23<00:37,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[157] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  80%|███████▉  | 159/200 [02:23<00:34,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[158] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  80%|████████  | 160/200 [02:24<00:35,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[159] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  80%|████████  | 161/200 [02:25<00:32,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[160] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  81%|████████  | 162/200 [02:26<00:30,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[161] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  82%|████████▏ | 163/200 [02:27<00:31,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[162] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  82%|████████▏ | 164/200 [02:28<00:29,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[163] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  82%|████████▎ | 165/200 [02:29<00:34,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[164] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  83%|████████▎ | 166/200 [02:30<00:34,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[165] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  84%|████████▎ | 167/200 [02:31<00:33,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[166] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  84%|████████▍ | 168/200 [02:32<00:31,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[167] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  84%|████████▍ | 169/200 [02:33<00:28,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[168] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  85%|████████▌ | 170/200 [02:34<00:27,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[169] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  86%|████████▌ | 171/200 [02:34<00:25,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[170] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  86%|████████▌ | 172/200 [02:35<00:24,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[171] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  86%|████████▋ | 173/200 [02:36<00:22,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[172] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  87%|████████▋ | 174/200 [02:37<00:22,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[173] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  88%|████████▊ | 175/200 [02:38<00:20,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[174] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  88%|████████▊ | 176/200 [02:39<00:19,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[175] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  88%|████████▊ | 177/200 [02:40<00:19,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[176] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  89%|████████▉ | 178/200 [02:40<00:17,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[177] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  90%|████████▉ | 179/200 [02:41<00:19,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[178] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  90%|█████████ | 180/200 [02:42<00:19,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[179] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  90%|█████████ | 181/200 [02:44<00:21,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[180] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  91%|█████████ | 182/200 [02:45<00:17,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[181] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  92%|█████████▏| 183/200 [02:46<00:16,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[182] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  92%|█████████▏| 184/200 [02:46<00:14,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[183] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  92%|█████████▎| 185/200 [02:47<00:13,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[184] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  93%|█████████▎| 186/200 [02:48<00:12,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[185] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  94%|█████████▎| 187/200 [02:49<00:10,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[186] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  94%|█████████▍| 188/200 [02:50<00:10,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[187] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  94%|█████████▍| 189/200 [02:50<00:09,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[188] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  95%|█████████▌| 190/200 [02:51<00:08,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[189] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  96%|█████████▌| 191/200 [02:52<00:07,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[190] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  96%|█████████▌| 192/200 [02:53<00:06,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[191] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  96%|█████████▋| 193/200 [02:54<00:05,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[192] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  97%|█████████▋| 194/200 [02:55<00:05,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[193] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  98%|█████████▊| 195/200 [02:56<00:05,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[194] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  98%|█████████▊| 196/200 [02:57<00:04,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[195] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  98%|█████████▊| 197/200 [02:58<00:03,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[196] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative:  99%|█████████▉| 198/200 [02:59<00:01,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[197] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rnegative: 100%|█████████▉| 199/200 [03:00<00:00,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[198] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "negative: 100%|██████████| 200/200 [03:01<00:00,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[199] victim shape: torch.Size([64]), variant shape: torch.Size([64])\n",
            "\n",
            "✅ Final ARUC (Linux SAGEDiff): 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ARUC scores for each model variant on NCI1 (Linux)\n",
        "variants = ['GCNMean', 'GCNDiff', 'SAGEMean', 'SAGEDiff']\n",
        "aruc_scores = [1.0000, 0.0000, 0.0000, 1.0000]\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(variants, aruc_scores)\n",
        "\n",
        "# Add score labels on top of each bar\n",
        "for bar, score in zip(bars, aruc_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, score + 0.02, f'{score:.4f}',\n",
        "             ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "# Styling\n",
        "plt.ylim(0, 1.1)\n",
        "plt.ylabel('ARUC Score')\n",
        "plt.title('ARUC Scores of GNN Variants on NCI1 (Linux Dataset)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Tnak9rLuWTFf",
        "outputId": "346926bf-8bf5-4d6f-b8d7-74bec91f3d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}